{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96a9939",
   "metadata": {},
   "source": [
    "# MultiVectorRetriever\n",
    "\n",
    "- Author: [YooKyung Jeon](https://github.com/sirena1)\n",
    "- Peer Review: [choincnp](https://github.com/choincnp), [Hye-yoonJeong](https://github.com/Hye-yoonJeong)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In LangChain, there's a special feature called `MultiVectorRetriever` that enables efficient querying of documents in various contexts. This feature allows documents to be stored and managed with multiple vectors, significantly enhancing the accuracy and efficiency of information retrieval.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environement Setup](#environment-setup)\n",
    "- [Methods for Generating Multiple Vectors Per Document](#methods-for-generating-multiple-vectors-per-document)\n",
    "- [Chunk + Original Document Retrieval](#chunk--original-document-retrieval)\n",
    "- [Storing summaries in vector storage](#storing-summaries-in-vector-storage)\n",
    "- [Utilizing Hypothetical Queries to explore document content](#utilizing-hypothetical-queries-to-explore-document-content)\n",
    "\n",
    "### References\n",
    "\n",
    "- [LangChain: Query Construction](https://blog.langchain.dev/query-construction/)\n",
    "- [LangGraph: Self-Reflective RAG](https://blog.langchain.dev/agentic-rag-with-langgraph/)\n",
    "- [Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity](https://arxiv.org/abs/2403.14403)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37cf0d8",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4952d",
   "metadata": {},
   "source": [
    "%%capture --no-stderr\n",
    "!pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29876d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain-chroma\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ec0472",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      3\u001b[0m load_dotenv(override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c356c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"09-FewShot-Prompt-Templates\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949f426",
   "metadata": {},
   "source": [
    "## Methods for Generating Multiple Vectors Per Document\n",
    "\n",
    "1. **Creating Small Chunks**: Divide the document into smaller chunks and generate separate embeddings for each chunk. This method enables a more granular focus on specific parts of the document. It can be implemented using the `ParentDocumentRetriever`, making it easier to explore detailed information.\n",
    "\n",
    "2. **Summary Embeddings**: Generate a summary for each document and create embeddings based on this summary. Summary embeddings are particularly useful for quickly grasping the core content of a document. By focusing only on the summary instead of analyzing the entire document, efficiency can be significantly improved.\n",
    "\n",
    "3. **Utilizing Hypothetical Questions**: Create relevant hypothetical questions for each document and generate embeddings based on these questions. This approach is helpful when deeper exploration of specific topics or content is needed. Hypothetical questions enable a broader perspective on the document's content, facilitating a more comprehensive understanding.\n",
    "\n",
    "4. **Manual Addition**: Users can manually add specific questions or queries that should be considered during document retrieval. This method provides users with more control over the search process, allowing for customized searches tailored to their specific needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b574ec5",
   "metadata": {},
   "source": [
    "The preprocessing process involves loading data from a text file and splitting the loaded documents into specified sizes.\n",
    "\n",
    "The split documents can later be used for tasks such as vectorization and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453afd1e",
   "metadata": {},
   "source": [
    "The original documents loaded from the data are stored in the docs variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a50910",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[5].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ef408",
   "metadata": {},
   "source": [
    "## Chunk + Original Document Retrieval\n",
    "\n",
    "When searching through large volumes of information, embedding data into smaller chunks can be highly beneficial.\n",
    "\n",
    "With `MultiVectorRetriever`, documents can be stored and managed as multiple vectors.\n",
    "\n",
    "- The original documents are stored in the `docstore`.\n",
    "- The embedded documents are stored in the `vectorstore`.\n",
    "\n",
    "This allows for splitting documents into smaller units, enabling more accurate searches. Additionally, the contents of the original document can be accessed when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404582fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# Vector store for indexing child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"small_bigger_chunks\",\n",
    "    embedding_function=OpenAIEmbeddings(model=\"text-embedding-ada-002\"),\n",
    ")\n",
    "\n",
    "# Storage layer for parent documents\n",
    "store = InMemoryByteStore()\n",
    "\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Retriever (initially empty)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Generate document IDs\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Verify two of the generated IDs\n",
    "print(doc_ids[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80961f",
   "metadata": {},
   "source": [
    "Defining `parent_text_splitter` for Larger Chunks and `child_text_splitter` for Smaller Chunks\n",
    "\n",
    "Here, we define `parent_text_splitter` for splitting into larger chunks and `child_text_splitter` for splitting into smaller chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc95363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RecursiveCharacterTextSplitter object for larger chunks\n",
    "parent_text_splitter = RecursiveCharacterTextSplitter(chunk_size=600)\n",
    "\n",
    "# Splitter to be used for generating smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e48a98",
   "metadata": {},
   "source": [
    "Create Parent documents as larger chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4437c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_docs = []\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    # Retrieve the ID of the current document\n",
    "    _id = doc_ids[i]\n",
    "    # Split the current document into smaller parent documents\n",
    "    parent_doc = parent_text_splitter.split_documents([doc])\n",
    "\n",
    "    for _doc in parent_doc:\n",
    "        # Store the document ID in the metadata\n",
    "        _doc.metadata[id_key] = _id\n",
    "    parent_docs.extend(parent_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5b0e5",
   "metadata": {},
   "source": [
    "Verify the `doc_id` assigned to `parent_docs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d7ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the metadata of the generated Parent documents.\n",
    "parent_docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f680da0",
   "metadata": {},
   "source": [
    "Create Child documents as relatively smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56afe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    # Retrieve the ID of the current document\n",
    "    _id = doc_ids[i]\n",
    "    # Split the current document into child documents\n",
    "    child_doc = child_text_splitter.split_documents([doc])\n",
    "    for _doc in child_doc:\n",
    "        # Store the document ID in the metadata\n",
    "        _doc.metadata[id_key] = _id\n",
    "    child_docs.extend(child_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe5cb5",
   "metadata": {},
   "source": [
    "Verify the `doc_id` assigned to `child_docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80857992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the metadata of the generated Child documents.\n",
    "child_docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a346a79",
   "metadata": {},
   "source": [
    "Check the number of chunks for each split document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of split parent_docs: {len(parent_docs)}\")\n",
    "print(f\"Number of split child_docs: {len(child_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6ab64",
   "metadata": {},
   "source": [
    "Add the newly created smaller child document set to the vector store\n",
    "\n",
    "Next, map the parent documents to the generated UUIDs and add them to the `docstore`.\n",
    "\n",
    "- Use the `mset()` method to store document IDs and their content as key-value pairs in the document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add both parent and child documents to the vector store\n",
    "retriever.vectorstore.add_documents(parent_docs)\n",
    "retriever.vectorstore.add_documents(child_docs)\n",
    "\n",
    "# Store the original documents in the docstore\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a644c1",
   "metadata": {},
   "source": [
    "Perform Similarity Search and Display the Most Similar Document Chunk\n",
    "\n",
    "Use the `retriever.vectorstore.similarity_search` method to search within child and parent document chunks.\n",
    "\n",
    "The first document chunk with the highest similarity will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d9ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform similarity search on the vectorstore\n",
    "relevant_chunks = retriever.vectorstore.similarity_search(\n",
    "    \"What is the name of the generative AI created by Samsung Electronics?\"\n",
    ")\n",
    "print(f\"Number of retrieved documents: {len(relevant_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in relevant_chunks:\n",
    "    print(chunk.page_content, end=\"\\n\\n\")\n",
    "    print(\">\" * 100, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f6f1a",
   "metadata": {},
   "source": [
    "Execute a Query Using the `retriever.invoke()` Method\n",
    "\n",
    "The `retriever.invoke()` method performs a search across the full content of the original documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = retriever.invoke(\n",
    "    \"What is the name of the generative AI created by Samsung Electronics?\"\n",
    ")\n",
    "print(f\"Number of retrieved documents: {len(relevant_docs)}\", end=\"\\n\\n\")\n",
    "print(\"=\" * 100, end=\"\\n\\n\")\n",
    "print(relevant_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc30896",
   "metadata": {},
   "source": [
    "The default search type performed by the retriever in the vector database is similarity search.\n",
    "\n",
    "LangChain Vector Stores also support searching using [Max Marginal Relevance](https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.VectorStore.html#langchain_core.vectorstores.VectorStore.max_marginal_relevance_search). \n",
    "\n",
    "If you want to use this method instead, you can configure the `search_type` property as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac718689",
   "metadata": {},
   "source": [
    "- Set the `search_type` property of the `retriever` object to `SearchType.mmr`.\n",
    "  - This specifies that the MMR (Maximal Marginal Relevance) algorithm should be used during the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "# Set the search type to Maximal Marginal Relevance (MMR)\n",
    "retriever.search_type = SearchType.mmr\n",
    "\n",
    "# Search all related documents\n",
    "print(\n",
    "    retriever.invoke(\n",
    "        \"What is the name of the generative AI created by Samsung Electronics?\"\n",
    "    )[0].page_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d08019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "# Set search type to similarity_score_threshold\n",
    "retriever.search_type = SearchType.similarity_score_threshold\n",
    "retriever.search_kwargs = {\"score_threshold\": 0.3}\n",
    "\n",
    "# Search all related documents\n",
    "print(\n",
    "    retriever.invoke(\n",
    "        \"What is the name of the generative AI created by Samsung Electronics?\"\n",
    "    )[0].page_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "# Set search type to similarity and k value to 1\n",
    "retriever.search_type = SearchType.similarity\n",
    "retriever.search_kwargs = {\"k\": 1}\n",
    "\n",
    "# Search all related documents\n",
    "print(\n",
    "    len(\n",
    "        retriever.invoke(\n",
    "            \"What is the name of the generative AI created by Samsung Electronics?\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d961b6c",
   "metadata": {},
   "source": [
    "## Storing summaries in vector storage\n",
    "\n",
    "Summaries can often provide a more accurate extraction of the contents of a chunk, which can lead to better search results.\n",
    "\n",
    "This section describes how to generate summaries and how to embed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c44d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for loading PDF files and splitting text\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the PDF file loader\n",
    "loader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "\n",
    "# Split text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=50)\n",
    "\n",
    "# Load a PDF file and run Text Split\n",
    "split_docs = loader.load_and_split(text_splitter)\n",
    "\n",
    "# Output the number of split documents\n",
    "print(f\"Number of split documents: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "summary_chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    # Create a prompt template for document summaries\n",
    "    | ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert in summarizing documents in Korean.\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Summarize the following documents in 3 sentences in bullet points format.\\n\\n{doc}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    # Using OpenAI's ChatGPT model to generate summaries\n",
    "    | ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b79165",
   "metadata": {},
   "source": [
    "Summarize the documents in the `docs` list in batch using the `chain.batch` method.\n",
    "- Here, we set the `max_concurrency` parameter to 10 to allow up to 10 documents to be processed simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling batches of documents\n",
    "summaries = summary_chain.batch(split_docs, {\"max_concurrency\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45363e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a11208",
   "metadata": {},
   "source": [
    "Print the summary to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436bd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the contents of the original document.\n",
    "print(split_docs[33].page_content, end=\"\\n\\n\")\n",
    "# Print a summary.\n",
    "print(\"[summary]\")\n",
    "print(summaries[33])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da5865",
   "metadata": {},
   "source": [
    "Initialize the `Chroma` vector store to index the child chunks. Use `OpenAIEmbeddings` as the embedding function.\n",
    "\n",
    "- Use `“doc_id”` as the key representing the document ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Create a vector store to store the summary information.\n",
    "summary_vectorstore = Chroma(\n",
    "    collection_name=\"summaries\",\n",
    "    embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    ")\n",
    "\n",
    "# Create a repository to store the parent document.\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Specify a key name to store the document ID.\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Initialize the searcher (empty at startup).\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=summary_vectorstore,  # vector store\n",
    "    byte_store=store,  # byte store\n",
    "    id_key=id_key,  # document ID\n",
    ")\n",
    "# Create a document ID.\n",
    "doc_ids = [str(uuid.uuid4()) for _ in split_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0e2c6",
   "metadata": {},
   "source": [
    "Save the summarized document and its metadata (here, the `Document ID` for the summary you created).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec99148",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_docs = [\n",
    "    # Create a Document object with the summary as the page content and the document ID as metadata.\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb07ac1",
   "metadata": {},
   "source": [
    "The number of articles in the digest matches the number of original articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e4d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents in the summary\n",
    "len(summary_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400de8c",
   "metadata": {},
   "source": [
    "- Add `summary_docs` to the vector store with `retriever.vectorstore.add_documents(summary_docs)`.\n",
    "- Map `doc_ids` and `docs` with `retriever.docstore.mset(list(zip(doc_ids, docs))))` to store them in the document store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ce3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(\n",
    "    summary_docs\n",
    ")  # Add the summarized document to the vector repository.\n",
    "\n",
    "# Map the document ID to the document and store it in the document store.\n",
    "retriever.docstore.mset(list(zip(doc_ids, split_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9158831",
   "metadata": {},
   "source": [
    "Perform a similarity search using the `similarity_search` method of the `vectorstore` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e3d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a similarity search.\n",
    "result_docs = summary_vectorstore.similarity_search(\n",
    "    \"What is the name of the generative AI created by Samsung Electronics?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7cb5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output 1 result document.\n",
    "print(result_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd64a80",
   "metadata": {},
   "source": [
    "Use the `invoke()` of the `retriever` object to retrieve documents related to your question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a9c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for and fetch related articles.\n",
    "retrieved_docs = retriever.invoke(\n",
    "    \"What is the name of the generative AI created by Samsung Electronics?\"\n",
    ")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d5f1b",
   "metadata": {},
   "source": [
    "## Utilizing Hypothetical Queries to explore document content\n",
    "\n",
    "LLM can also be used to generate a list of questions that can be hypothesized about a particular document.\n",
    "\n",
    "These generated questions can be embedded to further explore and understand the content of the document.\n",
    "\n",
    "Generating hypothetical questions can help you identify key topics and concepts in your documentation, and can encourage readers to ask more questions about the content of your documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7933e03",
   "metadata": {},
   "source": [
    "Below is an example of creating a hypothesis question utilizing `Function Calling`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"hypothetical_questions\",  # Specify a name for the function.\n",
    "        \"description\": \"Generate hypothetical questions\",  # Write a description of the function.\n",
    "        \"parameters\": {  # Define the parameters of the function.\n",
    "            \"type\": \"object\",  # Specifies the type of the parameter as an object.\n",
    "            \"properties\": {  # Defines the properties of an object.\n",
    "                \"questions\": {  # Define the 'questions' attribute.\n",
    "                    \"type\": \"array\",  # Type 'questions' as an array.\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },  # Specifies the array's element type as String.\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"questions\"],  # Specify 'questions' as a required parameter.\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa557a",
   "metadata": {},
   "source": [
    "Use `ChatPromptTemplate` to define a prompt template that generates three hypothetical questions based on the given document.\n",
    "\n",
    "- Set `functions` and `function_call` to call the virtual question generation functions.\n",
    "- Use `JsonKeyOutputFunctionsParser` to parse the generated virtual questions and extract the values corresponding to the `questions` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "hypothetical_query_chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    # We ask you to create exactly 3 hypothetical questions that you can answer using the documentation below. This number can be adjusted.\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer. \"\n",
    "        \"Potential users are those interested in the AI industry. Create questions that they would be interested in. \"\n",
    "        \"Output should be written in Korean:\\n\\n{doc}\"\n",
    "    )\n",
    "    | ChatOpenAI(max_retries=0, model=\"gpt-4o-mini\").bind(\n",
    "        functions=functions, function_call={\"name\": \"hypothetical_questions\"}\n",
    "    )\n",
    "    # Extract the value corresponding to the “questions” key from the output.\n",
    "    | JsonKeyOutputFunctionsParser(key_name=\"questions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b789211",
   "metadata": {},
   "source": [
    "Output the answers to the documents.\n",
    "\n",
    "- The output contains the three Hypothetical Queries you created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cda3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain for the given document.\n",
    "hypothetical_query_chain.invoke(split_docs[33])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ff912",
   "metadata": {},
   "source": [
    "Use the `chain.batch` method to process multiple requests for `split_docs` data at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac642190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of hypothetical questions for a list of articles\n",
    "hypothetical_questions = hypothetical_query_chain.batch(\n",
    "    split_docs, {\"max_concurrency\": 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02840bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_questions[33]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364ded0",
   "metadata": {},
   "source": [
    "Below is the process for storing the Hypothetical Queries you created in Vector Storage, the same way we did before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44404c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store to use for indexing child chunks\n",
    "hypothetical_vectorstore = Chroma(\n",
    "    collection_name=\"hypo-questions\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "# Storage hierarchy for parent documents\n",
    "store = InMemoryStore()\n",
    "\n",
    "id_key = \"doc_id\"\n",
    "# Retriever (empty on startup)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=hypothetical_vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in split_docs]  # Create a document ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8a6b7",
   "metadata": {},
   "source": [
    "Add metadata (document IDs) to the `question_docs` list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cbb65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_docs = []\n",
    "# save hypothetical_questions\n",
    "for i, question_list in enumerate(hypothetical_questions):\n",
    "    question_docs.extend(\n",
    "        # Create a Document object for each question in the list of questions, and include the document ID for that question in the metadata.\n",
    "        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a8e5f",
   "metadata": {},
   "source": [
    "Add the hypothesized query to the document, and add the original document to `docstore`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ecf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the hypothetical_questions document to the vector repository.\n",
    "retriever.vectorstore.add_documents(question_docs)\n",
    "\n",
    "# Map the document ID to the document and store it in the document store.\n",
    "retriever.docstore.mset(list(zip(doc_ids, split_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badb5835",
   "metadata": {},
   "source": [
    "Perform a similarity search using the `similarity_search` method of the `vectorstore` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9afb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the vector repository for similar documents.\n",
    "result_docs = hypothetical_vectorstore.similarity_search(\n",
    "    \"What is the name of the generative AI created by Samsung Electronics?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce41e6",
   "metadata": {},
   "source": [
    "Below are the results of the similarity search.\n",
    "\n",
    "Here, we've only added the hypothesized query we created, so it returns the documents with the highest similarity among the hypothesized queries we created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c33bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results of the similarity search.\n",
    "for doc in result_docs:\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88722c7",
   "metadata": {},
   "source": [
    "Use the `invoke` method of the `retriever` object to retrieve documents related to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for and fetch related articles.\n",
    "retrieved_docs = retriever.invoke(result_docs[1].page_content)\n",
    "\n",
    "# Output the documents found.\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
