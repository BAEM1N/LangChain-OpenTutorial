{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LangSmith Repeat Evaluation\n",
    "\n",
    "- Author: [Hwayoung Cha](https://github.com/forwardyoung)\n",
    "- Peer Review: []()\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "> Repetitive evaluation is a method of more accurately measuring a model's performance by conducting multiple evaluations on the same dataset.\n",
    "\n",
    "You can add repetition to the experiment. This notebook demonstrates how to use `LangSmith` for repeatable evaluations of language models. It covers setting up evaluation workflows, running evaluations on different datasets, and analyzing results to ensure consistency. The focus is on leveraging `LangSmith`'s tools for reproducible and scalable model assessments.\n",
    "\n",
    "This allows the evaluation to be repeated multiple times, which is useful in the following cases:\n",
    "\n",
    "- For larger evaluation sets\n",
    "- For chains that can generate variable responses\n",
    "- For evaluations that can produce variable scores (e.g., `llm-as-judge`)\n",
    "\n",
    "You can learn how to run an evaluation from [this site](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions).\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Performing Repetitive Evaluations with num_repetitions](#performing-repetitive-evaluations-with-num_repetitions)\n",
    "- [Define a function for RAG performance testing](#define-a-function-for-rag-performance-testing)\n",
    "- [Repetitive evaluation of RAG using GPT models](#repetitive-evaluation-of-rag-using-gpt-models)\n",
    "- [Repetitive evaluation of RAG using Ollama models](#repetitive-evaluation-of-rag-using-ollama-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions)\n",
    "- [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T15:37:00.758247Z",
     "start_time": "2025-01-16T15:36:50.388005Z"
    }
   },
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-16T15:37:00.761617Z"
    }
   },
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_ollama\",\n",
    "        \"faiss-cpu\"\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGSMITH_TRACING_V2\": \"true\",\n",
    "        \"LANGSMITH_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Repeat-Evaluations\"\n",
    "    }\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set OPENAI_API_KEY in .env file and load it.\n",
    "\n",
    "[Note] This is not necessary if you've already set OPENAI_API_KEY in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Configuration file to manage API keys as environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key information\n",
    "load_dotenv(override=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Repetitive Evaluations with `num_repetitions`\n",
    "\n",
    "LangSmith provides a simple way to perform repetitive evaluations using the `num_repetitions` parameter in the evaluate function. This parameter specifies how many times each example in your dataset should be evaluated.\n",
    "\n",
    "When you set `num_repetitions=N`, LangSmith will:\n",
    "\n",
    "Run each example in your dataset N times.\n",
    "\n",
    "Aggregate the results to provide a more accurate measure of your model's performance.\n",
    "\n",
    "For example:\n",
    "\n",
    "If your dataset has 10 examples and you set `num_repetitions=5`, each example will be evaluated 5 times, resulting in a total of 50 runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function for RAG performance testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from myrag import PDFRAG\n",
    "\n",
    "\n",
    "# Create a function to generate responses to questions.\n",
    "def ask_question_with_llm(llm):\n",
    "    # Create a PDFRAG object\n",
    "    rag = PDFRAG(\n",
    "        \"data/Newwhitepaper_Agents2.pdf\",\n",
    "        llm,\n",
    "    )\n",
    "\n",
    "    # Create a retriever\n",
    "    retriever = rag.create_retriever()\n",
    "\n",
    "    # Create a chain\n",
    "    rag_chain = rag.create_chain(retriever)\n",
    "\n",
    "    def _ask_question(inputs: dict):\n",
    "        # Context retrieval for the question\n",
    "        context = retriever.invoke(inputs[\"question\"])\n",
    "        # Combine the retrieved documents into a single string.\n",
    "        context = \"\\n\".join([doc.page_content for doc in context])\n",
    "        # Return a dictionary containing the question, context, and answer.\n",
    "        return {\n",
    "            \"question\": inputs[\"question\"],\n",
    "            \"context\": context,\n",
    "            \"answer\": rag_chain.invoke(inputs[\"question\"]),\n",
    "        }\n",
    "\n",
    "    return _ask_question"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we use the `llama3.2` model for repetitive evaluations. Below is an example of loading and invoking the model:  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Load the Ollama model\n",
    "ollama = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "# Call the Ollama model\n",
    "ollama.invoke(\"hello\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "gpt_chain = ask_question_with_llm(ChatOpenAI(model=\"gpt-4o-mini\", temperature=1.0))\n",
    "\n",
    "# Load the Ollama model.\n",
    "ollama_chain = ask_question_with_llm(ChatOllama(model=\"llama3.2\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repetitive evaluation of RAG using GPT models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# Create a QA evaluator\n",
    "cot_qa_evalulator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    config={\"llm\": ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)},\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"context\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate(\n",
    "    gpt_chain,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evalulator],\n",
    "    experiment_prefix=\"REPEAT_EVAL\",\n",
    "    # Specify the experiment metadata.\n",
    "    metadata={\n",
    "        \"variant\": \"Perform repeat evaluation. GPT-4o-mini model (cot_qa)\",\n",
    "    },\n",
    "    num_repetitions=3,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![13-LangSmith-Repeat-Evaluation-01](./assets/13-langSmith-repeat-evaluation-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repetitive evaluation of RAG using Ollama models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Create a QA evaluator\n",
    "cot_qa_evalulator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    config={\"llm\": ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)},\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"context\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate(\n",
    "    ollama_chain,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evalulator],\n",
    "    experiment_prefix=\"REPEAT_EVAL\",\n",
    "    # Specify the experiment metadata.\n",
    "    metadata={\n",
    "        \"variant\": \"Perform repeat evaluation. Ollama(llama3.2) (cot_qa)\",\n",
    "    },\n",
    "    num_repetitions=3,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-GHgbjDj7-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
