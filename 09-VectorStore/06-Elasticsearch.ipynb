{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a32a6ee5",
      "metadata": {},
      "source": [
        "# Elasticsearch\n",
        "\n",
        "- Author: [liniar](https://github.com/namyoungkim)\n",
        "- Design: \n",
        "- Peer Review: \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/08-Embeeding/03-HuggingFaceEmbeddings.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/08-Embeeding/03-HuggingFaceEmbeddings.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5799378",
      "metadata": {},
      "source": [
        "## Overview  \n",
        "- This tutorial explores how to implement keyword, semantic, and multimodal search using `Elasticsearch` as a vector database, seamlessly integrated with `LangChain`.\n",
        "- Elasticsearch offers powerful real-time search and analytics capabilities, while LangChain serves as a framework for integrating these capabilities into natural language processing and multimodal search applications.\n",
        "- Through this tutorial, we will implement the following using Elasticsearch and LangChain:\n",
        "    - 1Ô∏è‚É£ **Keyword Search**\n",
        "    - 2Ô∏è‚É£ **Semantic Search**\n",
        "    - 3Ô∏è‚É£ **Text-to-Image Multimodal Search**\n",
        "    - 4Ô∏è‚É£ **Image-to-Image Multimodal Search**\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d94e164",
      "metadata": {},
      "source": [
        "### Table of Contents  \n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Introduction to Elasticsearch](#introduction-to-elasticsearch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60b3831",
      "metadata": {},
      "source": [
        "### References\n",
        "- [LangChain VectorStore Documentation](https://python.langchain.com/docs/how_to/vectorstores/)\n",
        "- [LangChain Elasticsearch Integration](https://python.langchain.com/docs/integrations/vectorstores/elasticsearch/)\n",
        "- [Elasticsearch Official Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/index.html)  \n",
        "- [Elasticsearch Vector Search Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e7a3526",
      "metadata": {},
      "source": [
        "## Environment Setup  \n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.  \n",
        "\n",
        "**[Note]**  \n",
        "- `langchain-opentutorial` is a package that provides a set of **easy-to-use environment setup,** **useful functions,** and **utilities for tutorials.**  \n",
        "- You can check out the [`langchain-opentutorial` ](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.  \n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è **The following configurations will be set up**  \n",
        "\n",
        "- **Jupyter Notebook Output Settings**\n",
        "    - Display standard error ( `stderr` ) messages directly instead of capturing them.  \n",
        "- **Install Required Packages** \n",
        "    - Ensure all necessary dependencies are installed.  \n",
        "- **API Key Setup** \n",
        "    - Configure the API key for authentication.  \n",
        "- **PyTorch Device Selection Setup** \n",
        "    - Automatically select the optimal computing device (CPU, CUDA, or MPS).\n",
        "        - `{\"device\": \"mps\"}` : Perform embedding calculations using **MPS** instead of GPU. (For Mac users)\n",
        "        - `{\"device\": \"cuda\"}` : Perform embedding calculations using **GPU.** (For Linux and Windows users, requires CUDA installation)\n",
        "        - `{\"device\": \"cpu\"}` : Perform embedding calculations using **CPU.** (Available for all users)\n",
        "- **Embedding Model Local Storage Path** \n",
        "    - Define a local path for storing embedding models.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf93eed0",
      "metadata": {},
      "source": [
        "## Elasticsearch Setup\n",
        "- In order to use the Elasticsearch vector search you must install the langchain-elasticsearch package.\n",
        "\n",
        "### üöÄ Setting Up Elasticsearch with Elastic Cloud (Colab Compatible)\n",
        "- Elastic Cloud allows you to manage Elasticsearch seamlessly in the cloud, eliminating the need for local installations.\n",
        "- It integrates well with Google Colab, enabling efficient experimentation and prototyping.\n",
        "\n",
        "---\n",
        "\n",
        "### üìö What is Elastic Cloud?  \n",
        "- **Elastic Cloud** is a managed Elasticsearch service provided by Elastic.  \n",
        "- Supports **custom cluster configurations** and **auto-scaling**.  \n",
        "- Deployable on **AWS**, **GCP**, and **Azure**.  \n",
        "- Compatible with **Google Colab**, allowing simplified cloud-based workflows.  \n",
        "\n",
        "### üìå Getting Started with Elastic Cloud  \n",
        "1. **Sign up for Elastic Cloud‚Äôs Free Trial.**  \n",
        "    - [Free Trial](https://cloud.elastic.co/registration?utm_source=langchain&utm_content=documentation)\n",
        "2. **Create an Elasticsearch Cluster.**  \n",
        "3. **Retrieve your ES_URL** and **Elasticsearch API Key** from the Elastic Cloud Console.  \n",
        "    - Example ES_URL: `https://my-elasticsearch-project-abd...:123` \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21943adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain-core\",\n",
        "        \"langchain_openai\",\n",
        "        \"langchain_huggingface\",\n",
        "        \"langchain_elasticsearch\",\n",
        "        \"elasticsearch\",\n",
        "        \"python-dotenv\",\n",
        "        \"uuid\",\n",
        "        \"torch\",\n",
        "        \"langchain_experimental\",\n",
        "        \"open_clip_torch\",\n",
        "        \"pillow\",\n",
        "        \"pycocotools\",\n",
        "        \"numpy\",\n",
        "        \"matplotlib\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f9065ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set environment variables\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"OPENAI_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"Elasticsearch\",  # title Í≥º ÎèôÏùºÌïòÍ≤å ÏÑ§Ï†ïÌï¥ Ï£ºÏÑ∏Ïöî\n",
        "        \"HUGGINGFACEHUB_API_TOKEN\": \"\",\n",
        "        \"ES_URL\": \"\",\n",
        "        \"ES_API_KEY\": \"\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can alternatively set OPENAI_API_KEY in `.env` file and load it.\n",
        "\n",
        "**[Note]** \n",
        "- This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f99b5b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71b0e4a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Automatically select the appropriate device\n",
        "import torch\n",
        "import platform\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    if platform.system() == \"Darwin\":  # macOS specific\n",
        "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "            print(\"‚úÖ Using MPS (Metal Performance Shaders) on macOS\")\n",
        "            return \"mps\"\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"‚úÖ Using CUDA (NVIDIA GPU)\")\n",
        "        return \"cuda\"\n",
        "    else:\n",
        "        print(\"‚úÖ Using CPU\")\n",
        "        return \"cpu\"\n",
        "\n",
        "\n",
        "# Set the device\n",
        "device = get_device()\n",
        "print(\"üñ•Ô∏è Current device in use:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "647c0c07",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding Model Local Storage Path\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Ignore warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set the download path to ./cache/\n",
        "os.environ[\"HF_HOME\"] = \"./cache/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39dee13b",
      "metadata": {},
      "source": [
        "## Introduction to Elasticsearch\n",
        "- Elasticsearch is an open-source, distributed search and analytics engine designed to store, search, and analyze both structured and unstructured data in real-time.\n",
        "\n",
        "### üìå Key Features  \n",
        "- **Real-Time Search:** Instantly searchable data upon ingestion  \n",
        "- **Large-Scale Data Processing:** Efficient handling of vast datasets  \n",
        "- **Scalability:** Flexible scaling through clustering and distributed architecture  \n",
        "- **Versatile Search Support:** Keyword search, semantic search, and multimodal search  \n",
        "\n",
        "### üìå Use Cases  \n",
        "- **Log Analytics:** Real-time monitoring of system and application logs  \n",
        "- **Monitoring:** Server and network health tracking  \n",
        "- **Product Recommendations:** Behavior-based recommendation systems  \n",
        "- **Natural Language Processing (NLP):** Semantic text searches  \n",
        "- **Multimodal Search:** Text-to-image and image-to-image searches  \n",
        "\n",
        "### üß† Vector Database Functionality in Elasticsearch  \n",
        "- Elasticsearch supports vector data storage and similarity search via **Dense Vector Fields.** As a vector database, it excels in applications like NLP, image search, and recommendation systems.\n",
        "\n",
        "### üìå Core Vector Database Features  \n",
        "- **Dense Vector Field:** Store and query high-dimensional vectors  \n",
        "- **KNN (k-Nearest Neighbors) Search:** Find vectors most similar to the input  \n",
        "- **Semantic Search:** Perform meaning-based searches beyond keyword matching  \n",
        "- **Multimodal Search:** Combine text and image data for advanced search capabilities  \n",
        "\n",
        "### üìå Vector Search Use Cases  \n",
        "- **Semantic Search:** Understand user intent and deliver precise results  \n",
        "- **Text-to-Image Search:** Retrieve relevant images from textual descriptions  \n",
        "- **Image-to-Image Search:** Find visually similar images in a dataset  \n",
        "\n",
        "### üîó Official Documentation Links  \n",
        "- [Elasticsearch Official Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/index.html)  \n",
        "- [Elasticsearch Vector Search Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html)  \n",
        "\n",
        "Elasticsearch goes beyond traditional text search engines, offering robust vector database capabilities essential for NLP and multimodal search applications. üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fc94b411",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed907de0",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Ìå®ÌÇ§ÏßÄ Ï∂îÍ∞Ä ÏöîÏ≤≠Ìï† Í≤É\n",
        "%pip install -qU langchain-elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "414197d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "ES_URL = os.environ[\"ES_URL\"]           # Elasticsearch host URL\n",
        "ES_API_KEY = os.environ[\"ES_API_KEY\"]   # Elasticsearch API key\n",
        "\n",
        "# Ensure required environment variables are set\n",
        "if not ES_URL or not ES_API_KEY:\n",
        "    raise ValueError(\"Both ES_URL and ES_API_KEY must be set in environment variables.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503af97d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "# Elasticsearch ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉùÏÑ±\n",
        "es = Elasticsearch(\n",
        "    ES_URL,\n",
        "    api_key=ES_API_KEY\n",
        ")\n",
        "\n",
        "# Ïó∞Í≤∞ ÌôïÏù∏\n",
        "if es.ping():\n",
        "    print(\"‚úÖ Successfully connected to Elastic Cloud!\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to connect to Elastic Cloud.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "13b4acd4",
      "metadata": {},
      "outputs": [],
      "source": [
        "index_name=\"es_langchain_tutorial\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4924967b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_elasticsearch import ElasticsearchStore\n",
        "\n",
        "# Initialize ElasticsearchStore\n",
        "vector_store = ElasticsearchStore(\n",
        "    index_name=index_name,              # Elasticsearch index name\n",
        "    embedding=embeddings,               # Object responsible for text embeddings\n",
        "    es_url=ES_URL,                      # Elasticsearch host URL\n",
        "    es_api_key=ES_API_KEY,              # Elasticsearch API key for authentication\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "235c3398",
      "metadata": {},
      "source": [
        "## Manage vector store\n",
        "- Add items to vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a93db18d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e933c0c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "uuids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34c7a317",
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store.add_documents(documents=documents, ids=uuids)\n",
        "print(\"‚úÖ Documents added successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e64cb561",
      "metadata": {},
      "source": [
        "## Keyword Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d1f7604",
      "metadata": {},
      "outputs": [],
      "source": [
        "# keyword search query\n",
        "keyword=\"iPhone\"\n",
        "\n",
        "query = {\n",
        "    \"query\": {\n",
        "        \"match\": {\n",
        "            \"text\": keyword\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Elasticsearch Í≤ÄÏÉâ Ïã§Ìñâ\n",
        "results = es.search(index=index_name, body=query)\n",
        "print(\"üîç Keyword: \", keyword)\n",
        "print(\"‚úÖ Keyword Search Results:\")\n",
        "for hit in results['hits']['hits']:\n",
        "    print(f\"- {hit['_source']['text']} (Source: {hit['_source']['metadata']['source']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74e4b5e8",
      "metadata": {},
      "source": [
        "### üîë Explanation  \n",
        "- The **match query** is used to **search for documents** containing the keyword `\"iPhone\"` .  \n",
        "- The results will be returned with the following fields:  \n",
        "    - **text:** The content of the document.  \n",
        "    - **metadata:** Additional information or context related to the document.  \n",
        "\n",
        "This approach ensures that **documents containing the specified keyword** are **efficiently retrieved** along with their **relevant metadata**. üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22c39907",
      "metadata": {},
      "source": [
        "## Semantic Search\n",
        "- Similarity search\n",
        "    - Performing a simple similarity search with filtering on metadata can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91eb993d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Semantic Search Ïã§Ìñâ\n",
        "search_query = \"What's the weather like tomorrow?\"\n",
        "results = vector_store.similarity_search(search_query, k=3)\n",
        "\n",
        "print(\"üîç Question: \", search_query)\n",
        "print(\"ü§ñ Semantic Search Results:\")\n",
        "for result in results:\n",
        "    print(f\"- {result.page_content} (Source: {result.metadata['source']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "276bac88",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Semantic Search Ïã§Ìñâ\n",
        "search_query = \"LangChain provides abstractions to make working with LLMs easy\"\n",
        "results = vector_store.similarity_search(search_query, k=3)\n",
        "\n",
        "print(\"üîç Question: \", search_query)\n",
        "print(\"ü§ñ Semantic Search Results:\")\n",
        "for result in results:\n",
        "    print(f\"- {result.page_content} (Source: {result.metadata['source']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a5a5735",
      "metadata": {},
      "outputs": [],
      "source": [
        "# hybrid search\n",
        "search_query = \"LangChain provides abstractions to make working with LLMs easy\"\n",
        "keyword = \"tweet\"\n",
        "\n",
        "results = vector_store.similarity_search(\n",
        "    query=search_query,\n",
        "    k=2,\n",
        "    filter=[{\"term\": {\"metadata.source.keyword\": keyword}}],\n",
        ")\n",
        "\n",
        "print(\"üîç search_query: \", search_query)\n",
        "print(\"üîç keyword: \", keyword)\n",
        "\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63aa9d55",
      "metadata": {},
      "source": [
        "Similarity search with score\n",
        "- If you want to execute a similarity search and receive the corresponding scores you can run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2563e67d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# hybrid search with score\n",
        "search_query = \"Will it be hot tomorrow\"\n",
        "keyword = \"news\"\n",
        "\n",
        "\n",
        "results = vector_store.similarity_search_with_score(\n",
        "    query=search_query,\n",
        "    k=1,\n",
        "    filter=[{\"term\": {\"metadata.source.keyword\": keyword}}],\n",
        ")\n",
        "\n",
        "print(\"üîç search_query: \", search_query)\n",
        "print(\"üîç keyword: \", keyword)\n",
        "\n",
        "for doc, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a6b2df6",
      "metadata": {},
      "source": [
        "## Query by turning into retriever\n",
        "- You can also transform the vector store into a retriever for easier usage in your chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3278fab3",
      "metadata": {},
      "outputs": [],
      "source": [
        "search_query = \"Stealing from the bank is a crime\"\n",
        "print(\"üîç search_query: \", search_query)\n",
        "\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.2}\n",
        ")\n",
        "\n",
        "retriever.invoke(search_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8d9ab0",
      "metadata": {},
      "source": [
        "### Delete documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1b108f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete a Document by Specific UUID\n",
        "vector_store.delete(ids=[uuids[-1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd3ab344",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a query to match all documents\n",
        "query = {\n",
        "    \"query\": {\n",
        "        \"match_all\": {}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Execute the delete operation\n",
        "response = es.delete_by_query(index=index_name, body=query)\n",
        "print(\"üóëÔ∏è All documents have been successfully deleted!\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d21efba1",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install langchain_experimental pillow pycocotools open_clip_torch matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c95922a1",
      "metadata": {},
      "source": [
        "### Note\n",
        "- In this tutorial, while you have the option to define a new index_name for storing embeddings, we‚Äôll instead delete the existing index and reuse the same one for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84223691",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üÜï Define a new index name  \n",
        "new_index_name = \"es_langchain_tutorial_multimodal\"\n",
        "\n",
        "# üõ†Ô∏è Define the mapping for the new index  \n",
        "# This structure specifies the schema for documents stored in Elasticsearch\n",
        "mapping = {\n",
        "    \"mappings\": {\n",
        "        \"properties\": {\n",
        "            \"text\": {  # Field for storing textual content\n",
        "                \"type\": \"text\"\n",
        "            },\n",
        "            \"metadata\": {  # Field for storing metadata information\n",
        "                \"properties\": {\n",
        "                    \"image_path\": {  # Sub-field for image path metadata\n",
        "                        \"type\": \"text\"\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"vector\": {  # Field for storing vector embeddings\n",
        "                \"type\": \"dense_vector\",  # Specifies dense vector type\n",
        "                \"dims\": 1024,  # Number of dimensions in the vector\n",
        "                \"index\": True,  # Enable indexing for vector search\n",
        "                \"similarity\": \"cosine\"  # Use cosine similarity for vector comparisons\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# üöÄ Create a new index with the defined mapping  \n",
        "# Ïù∏Îç±Ïä§ Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏ Î∞è ÏÉùÏÑ±\n",
        "if not es.indices.exists(index=new_index_name):\n",
        "    es.indices.create(index=new_index_name, body=mapping)\n",
        "    print(f\"‚úÖ Index '{new_index_name}' created successfully.\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Index '{new_index_name}' already exists. Skipping creation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "049910bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "MODEL = \"ViT-H-14-quickgelu\"\n",
        "CHECKPOINT = \"dfn5b\"\n",
        "\n",
        "clip_embeddings = OpenCLIPEmbeddings(model_name=MODEL, checkpoint=CHECKPOINT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8461d79",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_elasticsearch import ElasticsearchStore\n",
        "\n",
        "# Initialize ElasticsearchStore\n",
        "vector_store = ElasticsearchStore(\n",
        "    index_name=new_index_name,                      # Elasticsearch index name\n",
        "    embedding=clip_embeddings,                      # Object responsible for text embeddings\n",
        "    es_url=ES_URL,                                  # Elasticsearch host URL\n",
        "    es_api_key=ES_API_KEY,                          # Elasticsearch API key for authentication\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Elasticsearch and OpenCLIP model configured successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47f34c3e",
      "metadata": {},
      "source": [
        "## Downloading the COCO Dataset (Subset)\n",
        "- The **COCO Dataset** is a widely-used multimodal dataset that includes **images paired with textual descriptions (captions).**\n",
        "- In this step, we‚Äôll **download a subset of the dataset** using the official API and **store it in the `./data` folder.** \n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Folder Creation & Dataset Download  \n",
        "\n",
        "### üîë Explanation:  \n",
        "- We‚Äôll **download both the COCO image dataset and the caption dataset.** \n",
        "- The datasets will be **saved in the `./data` folder`** and **automatically extracted for use.** \n",
        "\n",
        "This setup ensures that the **datasets are organized and ready for further processing. üöÄ‚ú®**  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "620a591a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# Define the directory for storing datasets\n",
        "DATA_DIR = \"./data\"\n",
        "COCO_API_URL = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
        "CAPTIONS_API_URL = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
        "\n",
        "# Create the dataset directory if it doesn't already exist\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Function to download a file from a given URL\n",
        "def download_file(url, save_path):\n",
        "    print(f\"üì• Downloading {url} ...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(save_path, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=1024):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "    print(f\"‚úÖ Saved to {save_path}\")\n",
        "\n",
        "# Download the COCO image dataset\n",
        "image_zip_path = os.path.join(DATA_DIR, \"val2017.zip\")\n",
        "download_file(COCO_API_URL, image_zip_path)\n",
        "\n",
        "# Download the COCO captions dataset\n",
        "captions_zip_path = os.path.join(DATA_DIR, \"annotations_trainval2017.zip\")\n",
        "download_file(CAPTIONS_API_URL, captions_zip_path)\n",
        "\n",
        "#Function to unzip downloaded files into a target directory\n",
        "def unzip_file(zip_path, extract_to):\n",
        "    print(f\"üì¶ Extracting {zip_path} ...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(f\"‚úÖ Extracted to {extract_to}\")\n",
        "\n",
        "unzip_file(image_zip_path, DATA_DIR)\n",
        "unzip_file(captions_zip_path, DATA_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9b39229",
      "metadata": {},
      "source": [
        "### üöÄ Indexing the COCO Dataset into Elasticsearch Using a Separate Storage Approach\n",
        "üõ†Ô∏è 3. Îç∞Ïù¥ÌÑ∞ Ïù∏Îç±Ïã±\n",
        "\n",
        "‚úÖ 3.1 Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
        "\n",
        "COCO Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÏòàÏ†úÎ°ú ÏÇ¨Ïö©Ìï©ÎãàÎã§."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e770d0b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pycocotools.coco import COCO\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ÏÖã ÏÑ§Ï†ï\n",
        "DATA_DIR = \"./data\"\n",
        "ANNOTATIONS_FILE = os.path.join(DATA_DIR, \"annotations\", \"captions_val2017.json\")\n",
        "IMAGES_DIR = os.path.join(DATA_DIR, \"val2017\")\n",
        "\n",
        "# COCO Ï∫°ÏÖò Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
        "coco = COCO(ANNOTATIONS_FILE)\n",
        "\n",
        "documents = []\n",
        "\n",
        "# Ï≤´ 100Í∞úÏùò Ïù¥ÎØ∏ÏßÄÏôÄ Ï∫°ÏÖò ÏÇ¨Ïö©\n",
        "for img_id in coco.getImgIds()[:100]:\n",
        "    img_info = coco.loadImgs(img_id)[0]\n",
        "    caption_ids = coco.getAnnIds(imgIds=img_id)\n",
        "    captions = [coco.loadAnns(c_id)[0]['caption'] for c_id in caption_ids]\n",
        "    \n",
        "    img_path = os.path.join(IMAGES_DIR, img_info['file_name'])\n",
        "    \n",
        "    for caption in captions:\n",
        "        documents.append({\n",
        "            \"text\": caption,\n",
        "            \"metadata\": {\"image_path\": img_path}\n",
        "        })\n",
        "    \n",
        "    documents.append({\n",
        "        \"text\": \"Image embedding\",\n",
        "        \"metadata\": {\"image_path\": img_path}\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50167e2b",
      "metadata": {},
      "source": [
        "‚úÖ 3.2 ÌÖçÏä§Ìä∏ Î∞è Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî© Ï†ÄÏû•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "020458bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import UnidentifiedImageError\n",
        "\n",
        "for doc in documents:\n",
        "    try:\n",
        "        if doc[\"text\"] == \"Image embedding\":\n",
        "            image_path = doc[\"metadata\"][\"image_path\"]\n",
        "            image_embedding = clip_embeddings.embed_image([image_path])[0]\n",
        "            vector_store.add_texts(\n",
        "                [doc[\"text\"]],\n",
        "                metadatas=[doc[\"metadata\"]],\n",
        "                vectors=[image_embedding]\n",
        "            )\n",
        "            print(f\"‚úÖ Image embedding added: {doc['metadata']['image_path']}\")\n",
        "        else:\n",
        "            text_embedding = clip_embeddings.embed_query(doc[\"text\"])\n",
        "            vector_store.add_texts(\n",
        "                [doc[\"text\"]],\n",
        "                metadatas=[doc[\"metadata\"]],\n",
        "                vectors=[text_embedding]\n",
        "            )\n",
        "            print(f\"‚úÖ Text embedding added: {doc['text']}\")\n",
        "    except UnidentifiedImageError:\n",
        "        print(f\"‚ùå Error: Unable to identify image at {doc['metadata']['image_path']}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: File not found - {doc['metadata']['image_path']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error while processing document: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be13df39",
      "metadata": {},
      "source": [
        "## Text-to-Image Multimodal Search\n",
        "- Returns the most relevant image based on the input text description.\n",
        "- This approach leverages vector embeddings to compare textual input with image metadata, enabling semantic similarity matching.\n",
        "\n",
        "This method is ideal for scenarios where users search for images using natural language descriptions. üöÄ‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2691319",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# %matplotlib inline setting (for running in Jupyter Notebook)\n",
        "%matplotlib inline\n",
        "\n",
        "# ‚úÖ Clear previous plots\n",
        "plt.close('all')\n",
        "\n",
        "# ‚úÖ Set the query text\n",
        "query_text = \"A man cooking food\"\n",
        "\n",
        "# ‚úÖ Generate query text embedding\n",
        "query_embedding = clip_embeddings.embed_query(query_text)\n",
        "\n",
        "# ‚úÖ Elasticsearch KNN search\n",
        "query_body = {\n",
        "    \"knn\": {\n",
        "        \"field\": \"vector\",\n",
        "        \"query_vector\": query_embedding,\n",
        "        \"k\": 5\n",
        "    }\n",
        "}\n",
        "\n",
        "results = es.search(index=new_index_name, body=query_body)\n",
        "\n",
        "# ‚úÖ Visualize search result images\n",
        "print(\"üîç Text-to-Image Search Results:\")\n",
        "fig, axes = plt.subplots(1, 6, figsize=(20, 5))\n",
        "\n",
        "# Display the query text (text cannot be displayed as an image, so it is shown in the title)\n",
        "axes[0].text(0.5, 0.5, query_text, fontsize=12, ha='center', va='center', wrap=True)\n",
        "axes[0].set_title(\"üîç Query Text\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "# Display similar images\n",
        "for i, hit in enumerate(results['hits']['hits']):\n",
        "    image_path = hit['_source']['metadata']['image_path']\n",
        "    caption = hit['_source']['text']\n",
        "    \n",
        "    try:\n",
        "        if os.path.exists(image_path):  # Check if the image path exists\n",
        "            result_image = Image.open(image_path)\n",
        "            axes[i+1].imshow(result_image)\n",
        "            axes[i+1].set_title(f\"Rank {i+1}\\n{caption[:30]}...\")\n",
        "            axes[i+1].axis(\"off\")\n",
        "        else:\n",
        "            axes[i+1].axis(\"off\")\n",
        "            axes[i+1].set_title(f\"‚ùå Image not found\\n{image_path}\")\n",
        "    except FileNotFoundError:\n",
        "        axes[i+1].axis(\"off\")\n",
        "        axes[i+1].set_title(f\"‚ùå Image not found\\n{image_path}\")\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d422526",
      "metadata": {},
      "source": [
        "## Image-to-Image Multimodal Search\n",
        "- Utilizes the `CLIP` model to generate vector embeddings from an input image.\n",
        "- Performs a similarity search in `Elasticsearch` to find images with matching vector representations.\n",
        "\n",
        "This approach enables highly accurate image retrieval based on visual content, making it ideal for tasks like finding visually similar images or identifying patterns across datasets. üöÄ‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc055357",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# %matplotlib inline setting (for running in Jupyter Notebook)\n",
        "%matplotlib inline\n",
        "\n",
        "# ‚úÖ Clear previous plots\n",
        "plt.close('all')\n",
        "\n",
        "# ‚úÖ Set the query image\n",
        "query_image_path = os.path.join(IMAGES_DIR, \"000000397133.jpg\")\n",
        "\n",
        "# ‚úÖ Generate query image embedding\n",
        "query_embedding = clip_embeddings.embed_image([query_image_path])[0]  # Pass the path\n",
        "\n",
        "# ‚úÖ Elasticsearch KNN search\n",
        "query_body = {\n",
        "    \"knn\": {\n",
        "        \"field\": \"vector\",\n",
        "        \"query_vector\": query_embedding,\n",
        "        \"k\": 5\n",
        "    }\n",
        "}\n",
        "\n",
        "results = es.search(index=new_index_name, body=query_body)\n",
        "\n",
        "# ‚úÖ Visualize search result images\n",
        "print(\"üîç Image-to-Image Search Results:\")\n",
        "fig, axes = plt.subplots(1, 6, figsize=(20, 5))\n",
        "\n",
        "# Display the query image\n",
        "axes[0].imshow(Image.open(query_image_path))\n",
        "axes[0].set_title(\"üîç Query Image\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "# Display similar images\n",
        "for i, hit in enumerate(results['hits']['hits']):\n",
        "    image_path = hit['_source']['metadata']['image_path']\n",
        "    caption = hit['_source']['text']\n",
        "    \n",
        "    try:\n",
        "        if os.path.exists(image_path):  # Check if the image path exists\n",
        "            result_image = Image.open(image_path)\n",
        "            axes[i+1].imshow(result_image)\n",
        "            axes[i+1].set_title(f\"Rank {i+1}\\n{caption[:30]}...\")\n",
        "            axes[i+1].axis(\"off\")\n",
        "        else:\n",
        "            axes[i+1].axis(\"off\")\n",
        "            axes[i+1].set_title(f\"‚ùå Image not found\\n{image_path}\")\n",
        "    except FileNotFoundError:\n",
        "        axes[i+1].axis(\"off\")\n",
        "        axes[i+1].set_title(f\"‚ùå Image not found\\n{image_path}\")\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7df8840",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a query to match all documents\n",
        "query = {\n",
        "    \"query\": {\n",
        "        \"match_all\": {}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Execute the delete operation\n",
        "response = es.delete_by_query(index=new_index_name, body=query)\n",
        "print(\"üóëÔ∏è All documents have been successfully deleted!\")\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-opentutorial-gmgjIYR5-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
