{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b82288e",
   "metadata": {},
   "source": [
    "# Exploring RAG in LangChain\n",
    "\n",
    "- Author: [Jaeho Kim](https://github.com/Jae-hoya)\n",
    "- Design: []()\n",
    "- Peer Review: [Sun Hyoung Lee](https://github.com/LEE1026icarus), [Sunyoung Park (architectyou)](https://github.com/Architectyou)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "![rag-1.png](./assets/12-rag-rag-basic-pdf-rag-process-01.png)\n",
    "\n",
    "![rag-2.png](./assets/12-rag-rag-basic-pdf-rag-process-02.png)\n",
    "\n",
    "## OverView\n",
    "\n",
    "This tutorial explores the entire process of indexing, retrieval, and generation using LangChain's RAG framework. It provides a broad overview of a typical RAG application pipeline and demonstrates how to effectively retrieve and generate responses by using LangChain's key features, such as data loaders, vector databases, embedding, retrievers, and generators, structured in a modular design.\n",
    "\n",
    "### 1. Question Processing\n",
    "\n",
    "The question processing stage involves receiving a user's question, handling it, and finding relevant data. The following components are required for this process:\n",
    "\n",
    "- **Data Source Connection**\n",
    "To find answers to the question, it is necessary to connect to various text data sources. LangChain helps you easily establish connections to various data sources.\n",
    "- **Data Indexing and Retrieval**\n",
    "To efficiently find relevant information from data sources, the data must be indexed. LangChain automates the indexing process and provides tools to retrieve data related to the user's question.\n",
    "\n",
    "\n",
    "### 2. Answer Generation\n",
    "\n",
    "Once the relevant data is found, the next step is to generate an answer based on it. The following components are essential for this stage:\n",
    "\n",
    "- **Answer Generation Model**\n",
    "LangChain uses advanced natural language processing (NLP) models to generate answers from the retrieved data. These models take the user's question and the retrieved data as input and generate an appropriate answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6f1b5",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "This Tutorial will build a typical RAG application as outlined in the [Q&A Introduction](https://python.langchain.com/docs/tutorials/). This consists of two main components:\n",
    "\n",
    "- **Indexing** : A pipeline that collects data from the source and indexes it. _This process typically occurs offline._\n",
    "\n",
    "- **Retrieval and Generation** : The actual RAG chain processes user queries in real-time, retrieves relevant data from the index, and passes it to the model.\n",
    "\n",
    "The entire workflow from raw data to generating an answer is as follows:\n",
    "\n",
    "### Indexing\n",
    "\n",
    "![](https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)\n",
    "\n",
    "1. **Load** : The first step is to load the data. For this, we will use [Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/).\n",
    "\n",
    "2. **Split** : [Text splitters](https://python.langchain.com/docs/concepts/text_splitters/) divide large `Documents` into smaller chunks.\n",
    "This is useful for indexing data and passing it to the model, as large chunks can be difficult to retrieve and may not fit within the model's limited context window.\n",
    "3. **Store** : The split data needs to be stored and indexed in a location for future retrieval. This is often accomplished using [VectorStore](https://python.langchain.com/docs/concepts/vectorstores/) and [Embeddings](https://python.langchain.com/docs/integrations/text_embedding/) Models.\n",
    "\n",
    "### Retrieval and Generation\n",
    "\n",
    "![](https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)\n",
    "\n",
    "1. **Retrieval** : When user input is provided, [Retriever](https://python.langchain.com/docs/integrations/retrievers/) is used to retrieve relevant chunks from the data store.\n",
    "2. **Generation** : [ChatModel](https://python.langchain.com/docs/integrations/chat/) / [LLM](https://python.langchain.com/docs/integrations/llms/) enerates an answer using a prompt that includes the question and the retrieved data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf05522",
   "metadata": {},
   "source": [
    "## Document Used for Practice\n",
    "\n",
    "A European Approach to Artificial Intelligence - A Policy Perspective\n",
    "\n",
    "- Author: Digital Enlightenment Forum under the guidance of EIT Digital, supported by contributions from EIT Manufacturing, EIT Urban Mobility, EIT Health, and EIT Climate-KIC\n",
    "- Link : https://eit.europa.eu/news-events/news/european-approach-artificial-intelligence-policy-perspective\n",
    "- File Name: `A European Approach to Artificial Intelligence - A Policy Perspective.pdf`\n",
    "\n",
    "_Please copy the downloaded file into the `data` folder for practice._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f999535",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Document Used for Practice](#document-used-for-practice)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Explore Each Module](#explore-each-module)\n",
    "- [Step 1: Load Document](#step-1:-load-document)\n",
    "- [Step 2: Split Documents](#step-2:-split-documents)\n",
    "- [Step 3: Embedding](#step-3:-embedding)\n",
    "- [Step 4: Create Vectorstore](#step-4-create-vectorstore)\n",
    "- [Step 5: Create Retriever ](#step-5-create-retriever)\n",
    "- [Step 6: Create Prompt](#step-6-create-prompt)\n",
    "- [Step 7: Create LLM](#step-7-create-llm)\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "- [LangChain: Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)\n",
    "- [LangChain: Text splitters](https://python.langchain.com/docs/concepts/text_splitters/)\n",
    "- [LangChain: Vector Store](https://python.langchain.com/docs/concepts/vectorstores/)\n",
    "- [LangChain: Embeddings](https://python.langchain.com/docs/integrations/text_embedding/)\n",
    "- [LangChain: Retriever](https://python.langchain.com/docs/integrations/retrievers/)\n",
    "- [LangChain: Chat Models](https://python.langchain.com/docs/integrations/chat/)\n",
    "- [LangChain: LLM](https://python.langchain.com/docs/integrations/llms/)\n",
    "- [Semantic Similarity Splitter: Greg Kamradt’s Notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb)\n",
    "- [OpenAI API Model List / Pricing](https://openai.com/api/pricing/)\n",
    "- [HuggingFace LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f12ae2",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcdd777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: c:\\Users\\skyop\\.pyenv\\pyenv-win\\versions\\3.11.9\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719a762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"bs4\",\n",
    "        \"faiss-cpu\",\n",
    "        \"pypdf\",\n",
    "        \"unstructured\",\n",
    "        \"unstructured[pdf]\",\n",
    "        \"chromadb\",\n",
    "        \"rank_bm25\",\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_experimental\"\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418ab505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"HUGGINGFACEHUB_API_TOKEN\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"02-RAG-Advanced\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3314b553",
   "metadata": {},
   "source": [
    "Environment variables have been set successfully.\n",
    "You can alternatively set API keys, such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f1b1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d050a",
   "metadata": {},
   "source": [
    "## Explore Each Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d1b0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e783c4",
   "metadata": {},
   "source": [
    "Below is an example of using a basic RAG model for handling web pages `(WebBaseLoader)` .\n",
    "\n",
    "In each step, you can configure various options or apply new techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377894c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.forbes.com/sites/rashishrivastava/2024/05/21/the-prompt-scarlett-johansson-vs-openai/\n",
      "Number of documents: 1\n",
      "============================================================\n",
      "[HUMAN]\n",
      "Why did OpenAI and Scarlett Johansson have a conflict?\n",
      "\n",
      "[AI]\n",
      "Scarlett Johansson and OpenAI had a conflict over a voice for ChatGPT that sounded similar to her own, which she claimed was created without her consent. After declining an offer to voice the AI, Johansson expressed shock and anger when the voice was used in a demo shortly thereafter. OpenAI later stated they would take down the voice, asserting it was not an imitation of Johansson.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Documents\n",
    "# Load the contents of news articles, split them into chunks, and index them.\n",
    "url = \"https://www.forbes.com/sites/rashishrivastava/2024/05/21/the-prompt-scarlett-johansson-vs-openai/\"\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url,),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"div\",\n",
    "            attrs={\"class\": [\"article-body fs-article fs-premium fs-responsive-text current-article font-body color-body bg-base font-accent article-subtype__masthead\",\n",
    "                             \"header-content-container masthead-header__container\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# Step 2: Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Step 3: Embedding & Create Vectorstore\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "# Step 4: retriever\n",
    "# Retrieve and generate information contained in the news.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Step 5: Create Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Step 6: Create LLM\n",
    "# Generate the language model (LLM).\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # Combine the retrieved document results into a single paragraph.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Create Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Step 8: Run Chain\n",
    "# Input queries about the documents and output answers.\n",
    "question = \"Why did OpenAI and Scarlett Johansson have a conflict?\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# output the results.\n",
    "print(f\"URL: {url}\")\n",
    "print(f\"Number of documents: {len(docs)}\")\n",
    "print(\"===\" * 20)\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7972a13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://www.forbes.com/sites/rashishrivastava/2024/05/21/the-prompt-scarlett-johansson-vs-openai/'}, page_content=\"ForbesInnovationEditors' PickThe Prompt: Scarlett Johansson Vs OpenAIPlus AI-generated kids draw predators on TikTok and Instagram. \\nShare to FacebookShare to TwitterShare to Linkedin“I was shocked, angered and in disbelief,” Scarlett Johansson said about OpenAI's Sky voice for ChatGPT that sounds similar to her own.FilmMagic\\nThe Prompt is a weekly rundown of AI’s buzziest startups, biggest breakthroughs, and business deals. To get it in your inbox, subscribe here.\\n\\n\\nWelcome back to The Prompt.\\n\\nScarlett Johansson’s lawyers have demanded that OpenAI take down a voice for ChatGPT that sounds much like her own after she’d declined to work with the company to create it. The actress said in a statement provided to Forbes that her lawyers have asked the AI company to detail the “exact processes” it used to create the voice, which sounds eerily similar to Johansson’s voiceover work in the sci-fi movie Her. “I was shocked, angered and in disbelief,” she said.\\n\\nThe actress said in the statement that last September Sam Altman offered to hire her to voice ChatGPT, adding that her voice would be comforting to people. She turned down the offer, citing personal reasons. Two days before OpenAI launched its latest model, GPT-4o, Altman reached out again, asking her to reconsider. But before she could respond, the voice was used in a demo, where it flirted, laughed and sang on stage. (“Oh stop it! You’re making me blush,” the voice said to the employee presenting the demo.)\\n\\nOn Monday, OpenAI said it would take down the voice, while claiming that it is not “an imitation of Scarlett Johansson” and that it had partnered with professional voice actors to create it. But Altman’s one-word tweet – “Her” – posted after the demo last week only further fueled the connection between the AI’s voice and Johannson’s.\\nNow, let’s get into the headlines.\\nBIG PLAYSActor and filmmaker Donald Glover tests out Google's new AI video tools.GOOGLE \\n\\nGoogle made a long string of AI-related announcements at its annual developer conference last week. The biggest one is that AI overviews — AI-generated summaries on any topic that will sit on top of search results — are rolling out to everyone across the U.S. But users were quick to express their frustration with the inaccuracies of these AI-generated snapshots. “90% of the results are pure nonsense or just incorrect,” one person wrote. “I literally might just stop using Google if I can't figure out how to turn off the damn AI overview,” another posted on X.\\nConsumers will also be able to use videos recorded with Google Lens to search for answers to questions like “What breed is this dog?” or “How do I fix this?” Plus, a new feature built on Gemini models will let them search their Google Photos gallery. Workspace products are getting an AI uplift as well: Google’s AI model Gemini 1.5 will let paying users find and summarize information in their Google Drive, Docs, Slides, Sheets and Gmail, and help generate content across these apps. Meanwhile, Google hired artists like actor and filmmaker Donald Glover and musician Wyclef Jean to promote Google’s new video and music creation AI tools.\\nDeepMind CEO Demis Hassabis touted Project Astra, a “universal assistant” that the company claims can see, hear and speak while understanding its surroundings. In a demo, the multimodel AI agent helps identify and fix pieces of code, create a band name and even find misplaced glasses.\\nTALENT RESHUFFLE\\nKey safety researchers at OpenAI, including cofounder and Chief Scientist Ilya Sutskever and machine learning researcher Jan Leike, have resigned. The two led the company’s efforts to develop ways to control AI systems that might become smarter than humans and prevent them from going rogue at the company’s superalignment team, which now no longer exists, according to Wired. In a thread on X, Leike wrote: “Over the past few months my team has been sailing against the wind. Sometimes we were struggling for compute and it was getting harder and harder to get this crucial research done. Over the past years, safety culture and processes have taken a backseat to shiny products.”\\nThe departure of these researchers also shone a light on OpenAI’s strict and binding nondisclosure agreements and off-boarding documents. Employees who refused to sign them when they left the company risked losing their vested equity in the company, according to Vox. OpenAI CEO Sam Altman responded on X saying “there was a provision about potential equity cancellation in our previous exit docs; although we never clawed anything back, it should never have been something we had in any documents or communication.”\\nAI DEALS OF THE WEEKAlexandr Wang was just 19 when he started Scale. His cofounder, Lucy Guo, was 21.Scale AI\\nScale AI has raised $1 billion at a $14 billion valuation in a round led by Accel. Amazon, Meta, Intel Capital and AMD Ventures are among the firm’s new investors. The company has hired hundreds of thousands of contractors in countries like Kenya and Venezuela through its in-house agency RemoTasks to complete data labeling tasks for training AI models, Forbes reported last year. In February, Forbes reported that the startup secretly scrapped a deal with TikTok amid national security concerns.\\nPlus: Coactive AI, which sorts through and structures a company’s visual data, has raised a $30 million round at a $200 million valuation led by Emerson Collective and Cherryrock Capital. And London-based PolyAI, which sells generative AI voice assistants for customer service and was cofounded by three machine learning PhD students at Cambridge, has raised $50 million at a nearly $500 million valuation.\\nDEEP DIVE Images of AI children on TikTok and Instagram are becoming magnets for many with a sexual interest in minors.ILLUSTRATION BY CECILIA RUNXI ZHANG; IMAGE BY ANTAGAIN/GETTY IMAGES\\nThe girls in the photos on TikTok and Instagram look like they could be 5 or 6 years old. On the older end, not quite 13. They’re pictured in lace and leather, bikinis and crop tops. They’re dressed suggestively as nurses, superheroes, ballerinas and french maids. Some wear bunny ears or devil horns; others, pigtails and oversized glasses. They’re black, white and Asian, blondes, redheads and brunettes. They were all made with AI, and they’ve become magnets for the attention of a troubling audience on some of the biggest social media apps in the world—older men.\\n“AI makes great works of art: I would like to have a pretty little virgin like that in my hands to make it mine,” one TikTok user commented on a recent post of young blonde girls in maid outfits, with bows around their necks and flowers in their hair.\\nSimilar remarks flooded photos of AI kids on Instagram. “I would love to take her innocence even if she’s a fake image,” one person wrote on a post of a small, pale child dressed as a bride. On another, of a young girl in short-shorts, the same user commented on “her cute pair of small size [breasts],” depicted as two apple emojis, “and her perfect innocent slice of cherry pie down below.”\\nForbes found hundreds of posts and comments like these on images of AI-generated kids on the platforms from 2024 alone. Many were tagged to musical hits—like Beyonce’s “Texas Hold ‘Em,” Taylor Swift’s “Shake It Off” and Tracy Chapman’s “Fast Car”—to help them reach more eyeballs.\\nChild predators have prowled most every major social media app—where they can hide behind screens and anonymous usernames—but TikTok and Instagram’s popularity with teens and minors has made them both top destinations. And though platforms’ struggle to crack down on child sexual abuse material (or CSAM) predates today’s AI boom, AI text-to-image generators are making it even easier for predators to find or create exactly what they’re looking for.\\nTikTok and Instagram permanently removed the accounts, videos and comments referenced in this story after Forbes asked about them; both companies said they violated platform rules.\\nRead the full story in Forbes here.\\nYOUR WEEKLY DEMO\\nOn Monday, Microsoft introduced a new line of Windows computers that have a suite of AI features built-in. Called “Copilot+ PCs”, the computers come equipped with AI-powered apps deployed locally on the device so you can run them without using an internet connection. The computers can record your screen to help you find anything you may have seen on it, generate images from text-based prompts and translate audio from 40 languages. Sold by brands like Dell, Lenovo and Samsung, the computers are able to do all this without internet access because their Qualcomm Snapdragon chips have a dedicated AI processor. The company claims its new laptops are about 60% faster and have 20% more battery life than Apple’s MacBook Air M3, and the first models will be on sale in mid-June.\\nMODEL BEHAVIOR\\nIn the past, universities have invited esteemed alumni to deliver commencement speeches at graduation ceremonies. This year, some institutions turned to AI. At D’Youville University in Buffalo, New York, a rather creepy-looking robot named Sophia delivered the commencement speech, doling out generic life lessons to an audience of 2,000 people. At Rensselaer Polytechnic Institute’s bicentennial graduation ceremony, GPT-4 was used to generate a speech from the perspective of Emily Warren Roebling, who helped complete the construction of the Brooklyn Bridge and received a posthumous degree from the university. The speech was read out by actress Liz Wisan.\\n\")]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b686d9",
   "metadata": {},
   "source": [
    "## Step 1: Load Document\n",
    "\n",
    "- [Link to official documentation - Document loaders](https://python.langchain.com/docs/integrations/document_loaders/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595217a5",
   "metadata": {},
   "source": [
    "### Web Page\n",
    "\n",
    "`WebBaseLoader` uses `bs4.SoupStrainer` to parse only the necessary parts from a specified web page.\n",
    "\n",
    "[Note]\n",
    "\n",
    "- `bs4.SoupStrainer` makes it convenient to extract desired elements from the web\n",
    "\n",
    "(example)\n",
    "\n",
    "```python\n",
    "bs4.SoupStrainer(\n",
    "    \"div\",\n",
    "    attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]}, # Input the class name.\n",
    ")\n",
    "\n",
    "bs4.SoupStrainer(\n",
    "    \"article\",\n",
    "    attrs={\"id\": [\"dic_area\"]}, # Input the class name.\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a040ec3",
   "metadata": {},
   "source": [
    "Here is another example, a BBC news article. Try running it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc118579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Could AI \\'trading bots\\' transform the world of investing?Getty ImagesIt is hard for both humans and computers to predict stock market movementsSearch for \"AI investing\" online, and you\\'ll be flooded with endless offers to let artificial intelligence manage your money.I recently spent half an hour finding out what so-called AI \"trading bots\" could apparently do with my investments.Many prominently suggest that they can give me lucrative returns. Yet as every reputable financial firm warns - your '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the contents of the news article, split it into chunks, and index it.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.bbc.com/news/business-68092814\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"main\",\n",
    "            attrs={\"id\": [\"main-content\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"Number of documents: {len(docs)}\")\n",
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43be1cf",
   "metadata": {},
   "source": [
    "### PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14f827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 24\n",
      "\n",
      "[page_content]\n",
      "A EUROPEAN APPROACH TO ARTIFICIAL INTELLIGENCE - A POLICY PERSPECTIVE\n",
      "10\n",
      "requirements becomes mandatory in all sectors and create bar -\n",
      "riers especially for innovators and SMEs. Public procurement ‘data \n",
      "sovereignty clauses’ induce large players to withdraw from AI for \n",
      "urban ecosystems. Strict liability sanctions block AI in healthcare, \n",
      "while limiting space of self-driving experimentation. The support \n",
      "measures to boost European AI are not sufficient to offset the \n",
      "unintended effect of generic\n",
      "\n",
      "[metadata]\n",
      "{'source': 'data/A European Approach to Artificial Intelligence - A Policy Perspective.pdf', 'page': 9}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF file. Enter the file path.\n",
    "loader = PyPDFLoader(\"data/A European Approach to Artificial Intelligence - A Policy Perspective.pdf\")\n",
    "\n",
    "\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"Number of documents: {len(docs)}\")\n",
    "\n",
    "# Output the content of the 10th page.\n",
    "print(f\"\\n[page_content]\\n{docs[9].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[9].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45216d9",
   "metadata": {},
   "source": [
    "### CSV\n",
    "\n",
    "CSV retrieves data using row numbers instead of page numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77419a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 20\n",
      "\n",
      "[row_content]\n",
      "PassengerId: 10\n",
      "Survived: 1\n",
      "Pclass: 2\n",
      "Name: Nasser, Mrs. Nicholas (Adele Achem)\n",
      "Sex: female\n",
      "Age: 14\n",
      "SibSp: 1\n",
      "Parch: 0\n",
      "Ticket: 237736\n",
      "Fare: 30.0708\n",
      "Cabin: \n",
      "Embarked: C\n",
      "\n",
      "[metadata]\n",
      "{'source': 'data/titanic.csv', 'row': 9}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# Load CSV file\n",
    "loader = CSVLoader(file_path=\"data/titanic.csv\")\n",
    "docs = loader.load()\n",
    "print(f\"Number of documents: {len(docs)}\")\n",
    "\n",
    "# Output the content of the 10th row.\n",
    "print(f\"\\n[row_content]\\n{docs[9].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[9].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4866c8ea",
   "metadata": {},
   "source": [
    "### TXT file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c00113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "\n",
      "[page_content]\n",
      "- Semantic Search\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching to understand the meaning of the user’s query and return relevant results.\n",
      "Example: When a user searches for \"planets in the solar system,\" it returns information about related planets such as \"Jupiter\" or \"Mars.\"\n",
      "Keywords: Natural Language Processing, Search Algorithm, Data Mining\n",
      "\n",
      "- Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words or sentences, int\n",
      "\n",
      "[metadata]\n",
      "{'source': 'data/appendix-keywords_eng.txt'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/appendix-keywords_eng.txt\")\n",
    "docs = loader.load()\n",
    "print(f\"Number of documents: {len(docs)}\")\n",
    "\n",
    "# Output the content of the 10th page.\n",
    "print(f\"\\n[page_content]\\n{docs[0].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[0].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567b4e0",
   "metadata": {},
   "source": [
    "### Load all files in the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133bf8dd",
   "metadata": {},
   "source": [
    "Here is an example of loading all `.txt` files in the folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb15de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      " 50%|█████     | 1/2 [00:02<00:02,  2.65s/it]libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2\n",
      "\n",
      "[page_content]\n",
      "Semantic Search\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching to understand the meaning of the user’s query and return relevant results. Example: When a user searches for \"planets in the solar system,\" it returns information about related planets such as \"Jupiter\" or \"Mars.\" Keywords: Natural Language Processing, Search Algorithm, Data Mining\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words or sentences, into lo\n",
      "\n",
      "[metadata]\n",
      "{'source': 'data\\\\appendix-keywords_eng.txt'}\n",
      "\n",
      "\n",
      "[metadata]\n",
      "{'source': 'data\\\\chain-of-density.txt'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\".\", glob=\"data/*.txt\", show_progress=True)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Number of documents: {len(docs)}\")\n",
    "\n",
    "# Output the content of the 10th page.\n",
    "print(f\"\\n[page_content]\\n{docs[0].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[0].metadata}\\n\")\n",
    "print(f\"\\n[metadata]\\n{docs[1].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9cbb02",
   "metadata": {},
   "source": [
    "The following is an example of loading all `.pdf` files in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc40246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\".\", glob=\"data/*.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"page_content: {len(docs)}\\n\")\n",
    "print(\"[metadata]\\n\")\n",
    "print(docs[0].metadata)\n",
    "print(\"\\n========= [Preview] Front Section =========\\n\")\n",
    "print(docs[0].page_content[2500:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3065d6",
   "metadata": {},
   "source": [
    "### Python\n",
    "\n",
    "The following is an example of loading `.py` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36678ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content: 1\n",
      "\n",
      "[metadata]\n",
      "\n",
      "{'source': 'data\\\\audio_utils.py'}\n",
      "\n",
      "========= [Preview] Front Section =========\n",
      "\n",
      "import re\n",
      "import os\n",
      "from pytube import YouTube\n",
      "from moviepy.editor import AudioFileClip, VideoFileClip\n",
      "from pydub import AudioSegment\n",
      "from pydub.silence import detect_nonsilent\n",
      "\n",
      "\n",
      "def extract_abr(abr):\n",
      "    youtube_audio_pattern = re.compile(r\"\\d+\")\n",
      "    kbps = youtube_audio_pattern.search(abr)\n",
      "    if kbps:\n",
      "        kbps = kbps.group()\n",
      "        return int(kbps)\n",
      "    else:\n",
      "        return 0\n",
      "\n",
      "\n",
      "def get_audio_filepath(filename):\n",
      "    # Create the audio folder if it doesn't exist\n",
      "    if not os.path.isdir(\"au\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PythonLoader\n",
    "\n",
    "loader = DirectoryLoader(\".\", glob=\"**/*.py\", loader_cls=PythonLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"page_content: {len(docs)}\\n\")\n",
    "print(\"[metadata]\\n\")\n",
    "print(docs[0].metadata)\n",
    "print(\"\\n========= [Preview] Front Section =========\\n\")\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505bd21b",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0321a46",
   "metadata": {},
   "source": [
    "## Step 2: Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47ad4770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Could AI \\'trading bots\\' transform the world of investing?Getty ImagesIt is hard for both humans and computers to predict stock market movementsSearch for \"AI investing\" online, and you\\'ll be flooded with endless offers to let artificial intelligence manage your money.I recently spent half an hour finding out what so-called AI \"trading bots\" could apparently do with my investments.Many prominently suggest that they can give me lucrative returns. Yet as every reputable financial firm warns - your '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the content of the news article, split it into chunks, and index it.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.bbc.com/news/business-68092814\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"main\",\n",
    "            attrs={\"id\": [\"main-content\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"Number of Documents: {len(docs)}\")\n",
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc11cb",
   "metadata": {},
   "source": [
    "### CharacterTextSplitter\n",
    "\n",
    "This is the simplest method. It splits the text based on characters (default: \"\\n\\n\") and measures the chunk size by the number of characters.\n",
    "\n",
    "1. **How the text is split** : By single character units.\n",
    "2. **How the chunk size is measured** : By the `len` of characters.\n",
    "\n",
    "Visualization example: https://chunkviz.up.railway.app/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4a799",
   "metadata": {},
   "source": [
    "The `CharacterTextSplitter` class provides functionality to split text into chunks of a specified size.\n",
    "\n",
    "- `separator` parameter specifies the string used to separate chunks, with two newline characters (\"\\n\\n\") being used in this case.\n",
    "- `chunk_size`determines the maximum length of each chunk.\n",
    "- `chunk_overlap`specifies the number of overlapping characters between adjacent chunks.\n",
    "- `length_function`defines the function used to calculate the length of a chunk, with the default being the `len` function, which returns the length of the string.\n",
    "- `is_separator_regex`is a boolean value that determines whether the `separator` is interpreted as a regular expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ade83078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d7e0d",
   "metadata": {},
   "source": [
    "This function uses the `create_documents` method of the `text_splitter` object to split the given text (`state_of_the_union`) into multiple documents, storing the results in the `texts` variable. It then outputs the first document from texts. This process can be seen as an initial step for processing and analyzing text data, particularly useful for splitting large text data into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "472bc3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a portion of the \"Chain of Density\" paper.\n",
    "with open(\"data/chain-of-density.txt\", \"r\") as f:\n",
    "    text = f.read()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70e11f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selecting the “right” amount of information to include in a summary is a difficult task. \\nA good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries genera']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10, separator=\"\\n\\n\"\n",
    ")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24506d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selecting the “right” amount of information to include in a summary is a difficult task.',\n",
       " 'A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries genera']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=\"\\n\")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57da16a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selecting the “right” amount of information to include in a summary is a difficult task. \\nA good',\n",
       " 'A good summary should be detailed and entity-centric without being overly dense and hard to follow.',\n",
       " 'to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with',\n",
       " 'with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial',\n",
       " 'an initial entity-sparse summary before iteratively incorporating missing salient entities without',\n",
       " 'without increasing the length. Summaries genera']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=\" \")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c743226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selecting the “right” amount of information to include in a summary is a difficult task. \\nA good',\n",
       " 'summary should be detailed and entity-centric without being overly dense and hard to follow. To',\n",
       " 'better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to',\n",
       " 'as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary',\n",
       " 'before iteratively incorporating missing salient entities without increasing the length. Summaries',\n",
       " 'genera']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0, separator=\" \")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b5b02d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\" \")\n",
    "# Split the text file into chunks.\n",
    "text_splitter.split_text(text)\n",
    "\n",
    "# Split the document into chunks.\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2f39fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.bbc.com/news/business-68092814'}, page_content='Could AI \\'trading bots\\' transform the world of investing?Getty ImagesIt is hard for both humans and computers to predict stock market movementsSearch for \"AI investing\" online, and you\\'ll be flooded with endless offers to let artificial intelligence manage your money.I recently spent half an hour finding out what so-called AI \"trading bots\" could apparently do with my investments.Many prominently suggest that they can give me lucrative returns. Yet as every reputable financial firm warns - your capital may be at risk.Or putting it more simply - you could lose your money - whether it is a human or a computer that is making stock market decisions on your behalf.Yet such has been the hype about the ability of AI over the past few years, that almost one in three investors would be happy to let a trading bot make all the decisions for them, according to one 2023 survey in the US.John Allan says investors should be more cautious about using AI. He is head of innovation and operations for the')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e376c2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Could AI \\'trading bots\\' transform the world of investing?Getty ImagesIt is hard for both humans and computers to predict stock market movementsSearch for \"AI investing\" online, and you\\'ll be flooded with endless offers to let artificial intelligence manage your money.I recently spent half an hour finding out what so-called AI \"trading bots\" could apparently do with my investments.Many prominently suggest that they can give me lucrative returns. Yet as every reputable financial firm warns - your '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the content of the news article, split it into chunks, and index it.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.bbc.com/news/business-68092814\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"main\",\n",
    "            attrs={\"id\": [\"main-content\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define the splitter.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\" \")\n",
    "\n",
    "# Split the document while loading it.\n",
    "split_docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "print(f\"Number of documents: {len(docs)}\")\n",
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3662cd",
   "metadata": {},
   "source": [
    "### RecursiveTextSplitter\n",
    "This text splitter is recommended for general text.\n",
    "\n",
    "1. `How the text is split` : Based on a list of separators.\n",
    "2. `How the chunk size is measured` : By the len of characters.\n",
    "\n",
    "The `RecursiveCharacterTextSplitter` class provides functionality to recursively split text. This class takes parameters such as `chunk_size` to specify the size of the chunks to be split, `chunk_overlap` to define the overlap size between adjacent chunks, length_function to calculate the length of the chunks, and `is_separator_regex` to indicate whether the separator is a regular expression.\n",
    "\n",
    "In the example, the chunk size is set to 100, the overlap size to 20, the length calculation function to `len` , and `is_separator_regex` is set to `False` to indicate that the separator is not a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fc0587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a8cf631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a portion of the \"Chain of Density\" paper.\n",
    "with open(\"data/chain-of-density.txt\", \"r\") as f:\n",
    "    text = f.read()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26f2bb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting the “right” amount of information to include in a summary is a difficult task. \n",
      "A good\n",
      "A good summary should be detailed and entity-centric without being overly dense and hard to follow.\n",
      "to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with\n",
      "with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial\n",
      "an initial entity-sparse summary before iteratively incorporating missing salient entities without\n",
      "without increasing the length. Summaries genera\n",
      "============================================================\n",
      "Selecting the “right” amount of information to include in a summary is a difficult task.\n",
      "A good summary should be detailed and entity-centric without being overly dense and hard to follow.\n",
      "follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what\n",
      "with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an\n",
      "an initial entity-sparse summary before iteratively incorporating missing salient entities without\n",
      "without increasing the length. Summaries genera\n"
     ]
    }
   ],
   "source": [
    "character_text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10, separator=\" \"\n",
    ")\n",
    "for sent in character_text_splitter.split_text(text):\n",
    "    print(sent)\n",
    "print(\"===\" * 20)\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10\n",
    ")\n",
    "for sent in recursive_text_splitter.split_text(text):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264cf4ae",
   "metadata": {},
   "source": [
    "- Attempts to split the given document sequentially using the specified list of separators.\n",
    "- Attempts splitting in order until the chunks are sufficiently small. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"].\n",
    "- This generally has the effect of keeping all paragraphs (as well as sentences and words) as long as possible, while appearing to be the most semantically relevant pieces of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "393ec4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the default separators specified in recursive_text_splitter.\n",
    "recursive_text_splitter._separators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49d975",
   "metadata": {},
   "source": [
    "### Semantic Similarity\n",
    "\n",
    "Text is split based on semantic similarity.\n",
    "\n",
    "Source: [Greg Kamradt’s Notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb)\n",
    "\n",
    "At a high level, the process involves splitting the text into sentences, grouping them into sets of three, and then merging similar sentences in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dccff243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Create a SemanticChunker.\n",
    "semantic_text_splitter = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-small\"), add_start_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a27b2d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting the “right” amount of information to include in a summary is a difficult task. A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries generated by CoD are more abstractive, exhibit more fusion, and have less of a lead bias than GPT-4 summaries generated by a vanilla prompt. We conduct a human preference study on 100 CNN DailyMail articles and find that that humans prefer GPT-4 summaries that are more dense than those generated by a vanilla prompt and almost as dense as human written summaries. Qualitative analysis supports the notion that there exists a tradeoff between infor-mativeness and readability. 500 annotated CoD summaries, as well as an extra 5,000 unannotated summaries, are freely available on HuggingFace. Introduction\n",
      "\n",
      "Automatic summarization has come a long way in the past few years, largely due to a paradigm shift away from supervised fine-tuning on labeled datasets to zero-shot prompting with Large Language Models (LLMs), such as GPT-4 (OpenAI, 2023). Without additional training, careful prompting can enable fine-grained control over summary characteristics, such as length (Goyal et al., 2022), topics (Bhaskar et al., 2023), and style (Pu and Demberg, 2023).\n",
      "============================================================\n",
      "An overlooked aspect is the information density of an summary. In theory, as a compression of another text, a summary should be denser–containing a higher concentration of information–than the source document. Given the high latency of LLM decoding (Kad-dour et al., 2023), covering more information in fewer words is a worthy goal, especially for real-time applications. Yet, how dense is an open question. A summary is uninformative if it contains insufficient detail. If it contains too much information, however, it can be-come difficult to follow without having to increase the overall length. Conveying more information subject to a fixed token budget requires a combination of abstrac-tion, compression, and fusion. There is a limit to how much space can be made for additional information before becoming illegible or even factually incorrect.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load a portion of the \"Chain of Density\" paper.\n",
    "with open(\"data/chain-of-density.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "for sent in semantic_text_splitter.split_text(text):\n",
    "    print(sent)\n",
    "    print(\"===\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f3467",
   "metadata": {},
   "source": [
    "## Step 3: Embedding\n",
    "\n",
    "- [Link to official documentation - Embedding](https://python.langchain.com/docs/integrations/text_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9affbaf8",
   "metadata": {},
   "source": [
    "### Paid Embeddings (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccc62a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Step 3: Create Embeddings & Vectorstore\n",
    "# Generate the vector store.\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f65e243",
   "metadata": {},
   "source": [
    "Below is a list of Embedding models supported by `OpenAI` :\n",
    "\n",
    "The default model is `text-embedding-ada-002` .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c021b1",
   "metadata": {},
   "source": [
    "| MODEL                  | ROUGH PAGES PER DOLLAR | EXAMPLE PERFORMANCE ON MTEB EVAL |\n",
    "| ---------------------- | ---------------------- | -------------------------------- |\n",
    "| text-embedding-3-small | 62,500                 | 62.3%                            |\n",
    "| text-embedding-3-large | 9,615                  | 64.6%                            |\n",
    "| text-embedding-ada-002 | 12,500                 | 61.0%                            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d07fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cf6e8",
   "metadata": {},
   "source": [
    "### Free Open Source-Based Embeddings\n",
    "1. HuggingFaceBgeEmbeddings\n",
    "2. FastEmbedEmbeddings\n",
    "\n",
    "**Note**\n",
    "- When using embeddings, make sure to verify that the language you are using is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ca55cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_35972\\2680422856.py:5: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  documents=splits, embedding=HuggingFaceBgeEmbeddings()\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_35972\\2680422856.py:5: LangChainDeprecationWarning: Default values for HuggingFaceBgeEmbeddings.model_name were deprecated in LangChain 0.2.5 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceBgeEmbeddings constructor instead.\n",
      "  documents=splits, embedding=HuggingFaceBgeEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "# Generate the vector store.\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits, embedding=HuggingFaceBgeEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4642c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d13003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=FastEmbedEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd42083c",
   "metadata": {},
   "source": [
    "## Step 4: Create Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9033e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Apply FAISS DB\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0a99a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Apply Chroma DB\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8150225",
   "metadata": {},
   "source": [
    "## Step 5: Create Retriever \n",
    "\n",
    "A Retriever is an interface that returns documents when given an unstructured query.\n",
    "\n",
    "The Retriever does not need to store documents; it only returns (or retrieves) them.\n",
    "\n",
    "- [Link to official documentation - Retriever](https://python.langchain.com/docs/integrations/retrievers/)\n",
    "\n",
    "The **Retriever** is created by using the `as_retriever()` method on the generated VectorStore.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90798e85",
   "metadata": {},
   "source": [
    "### Similarity Retrieval\n",
    "\n",
    "- The default setting is `similarity` , which uses cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2bd6d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://www.forbes.com/sites/rashishrivastava/2024/05/21/the-prompt-scarlett-johansson-vs-openai/'}, page_content=\"ForbesInnovationEditors' PickThe Prompt: Scarlett Johansson Vs OpenAIPlus AI-generated kids draw predators on TikTok and Instagram. \\nShare to FacebookShare to TwitterShare to Linkedin“I was shocked, angered and in disbelief,” Scarlett Johansson said about OpenAI's Sky voice for ChatGPT that sounds similar to her own.FilmMagic\\nThe Prompt is a weekly rundown of AI’s buzziest startups, biggest breakthroughs, and business deals. To get it in your inbox, subscribe here.\\n\\n\\nWelcome back to The Prompt.\\n\\nScarlett Johansson’s lawyers have demanded that OpenAI take down a voice for ChatGPT that sounds much like her own after she’d declined to work with the company to create it. The actress said in a statement provided to Forbes that her lawyers have asked the AI company to detail the “exact processes” it used to create the voice, which sounds eerily similar to Johansson’s voiceover work in the sci-fi movie Her. “I was shocked, angered and in disbelief,” she said.\"), Document(metadata={'source': 'https://www.forbes.com/sites/rashishrivastava/2024/05/21/the-prompt-scarlett-johansson-vs-openai/'}, page_content=\"The actress said in the statement that last September Sam Altman offered to hire her to voice ChatGPT, adding that her voice would be comforting to people. She turned down the offer, citing personal reasons. Two days before OpenAI launched its latest model, GPT-4o, Altman reached out again, asking her to reconsider. But before she could respond, the voice was used in a demo, where it flirted, laughed and sang on stage. (“Oh stop it! You’re making me blush,” the voice said to the employee presenting the demo.)\\n\\nOn Monday, OpenAI said it would take down the voice, while claiming that it is not “an imitation of Scarlett Johansson” and that it had partnered with professional voice actors to create it. But Altman’s one-word tweet – “Her” – posted after the demo last week only further fueled the connection between the AI’s voice and Johannson’s.\\nNow, let’s get into the headlines.\\nBIG PLAYSActor and filmmaker Donald Glover tests out Google's new AI video tools.GOOGLE\"), Document(metadata={'source': 'https://www.forbes.com/sites/rashishrivastava/2024/05/21/the-prompt-scarlett-johansson-vs-openai/'}, page_content='The departure of these researchers also shone a light on OpenAI’s strict and binding nondisclosure agreements and off-boarding documents. Employees who refused to sign them when they left the company risked losing their vested equity in the company, according to Vox. OpenAI CEO Sam Altman responded on X saying “there was a provision about potential equity cancellation in our previous exit docs; although we never clawed anything back, it should never have been something we had in any documents or communication.”\\nAI DEALS OF THE WEEKAlexandr Wang was just 19 when he started Scale. His cofounder, Lucy Guo, was 21.Scale AI'), Document(metadata={'source': 'https://www.forbes.com/sites/rashishrivastava/2024/05/21/the-prompt-scarlett-johansson-vs-openai/'}, page_content='TALENT RESHUFFLE\\nKey safety researchers at OpenAI, including cofounder and Chief Scientist Ilya Sutskever and machine learning researcher Jan Leike, have resigned. The two led the company’s efforts to develop ways to control AI systems that might become smarter than humans and prevent them from going rogue at the company’s superalignment team, which now no longer exists, according to Wired. In a thread on X, Leike wrote: “Over the past few months my team has been sailing against the wind. Sometimes we were struggling for compute and it was getting harder and harder to get this crucial research done. Over the past years, safety culture and processes have taken a backseat to shiny products.”')]\n"
     ]
    }
   ],
   "source": [
    "question = \"Why did OpenAI and Scarlett Johansson have a conflict?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "search_result = retriever.invoke(question)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107dea5",
   "metadata": {},
   "source": [
    "The `similarity_score_threshold` returns only the results with a `score_threshold` or higher in similarity-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "067f05aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://www.forbes.com/sites/rashishrivastava/2024/05/21/the-prompt-scarlett-johansson-vs-openai/'}, page_content=\"ForbesInnovationEditors' PickThe Prompt: Scarlett Johansson Vs OpenAIPlus AI-generated kids draw predators on TikTok and Instagram. \\nShare to FacebookShare to TwitterShare to Linkedin“I was shocked, angered and in disbelief,” Scarlett Johansson said about OpenAI's Sky voice for ChatGPT that sounds similar to her own.FilmMagic\\nThe Prompt is a weekly rundown of AI’s buzziest startups, biggest breakthroughs, and business deals. To get it in your inbox, subscribe here.\\n\\n\\nWelcome back to The Prompt.\\n\\nScarlett Johansson’s lawyers have demanded that OpenAI take down a voice for ChatGPT that sounds much like her own after she’d declined to work with the company to create it. The actress said in a statement provided to Forbes that her lawyers have asked the AI company to detail the “exact processes” it used to create the voice, which sounds eerily similar to Johansson’s voiceover work in the sci-fi movie Her. “I was shocked, angered and in disbelief,” she said.\")]\n"
     ]
    }
   ],
   "source": [
    "question = \"Why did OpenAI and Scarlett Johansson have a conflict?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.8}\n",
    ")\n",
    "search_result = retriever.invoke(question)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72fee8",
   "metadata": {},
   "source": [
    "Search using the `maximum marginal search result(mmr)` .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "405337a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 12, updating n_results = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://www.forbes.com/sites/rashishrivastava/2024/05/21/the-prompt-scarlett-johansson-vs-openai/'}, page_content=\"ForbesInnovationEditors' PickThe Prompt: Scarlett Johansson Vs OpenAIPlus AI-generated kids draw predators on TikTok and Instagram. \\nShare to FacebookShare to TwitterShare to Linkedin“I was shocked, angered and in disbelief,” Scarlett Johansson said about OpenAI's Sky voice for ChatGPT that sounds similar to her own.FilmMagic\\nThe Prompt is a weekly rundown of AI’s buzziest startups, biggest breakthroughs, and business deals. To get it in your inbox, subscribe here.\\n\\n\\nWelcome back to The Prompt.\\n\\nScarlett Johansson’s lawyers have demanded that OpenAI take down a voice for ChatGPT that sounds much like her own after she’d declined to work with the company to create it. The actress said in a statement provided to Forbes that her lawyers have asked the AI company to detail the “exact processes” it used to create the voice, which sounds eerily similar to Johansson’s voiceover work in the sci-fi movie Her. “I was shocked, angered and in disbelief,” she said.\"), Document(metadata={'source': 'https://www.forbes.com/sites/rashishrivastava/2024/05/21/the-prompt-scarlett-johansson-vs-openai/'}, page_content='The departure of these researchers also shone a light on OpenAI’s strict and binding nondisclosure agreements and off-boarding documents. Employees who refused to sign them when they left the company risked losing their vested equity in the company, according to Vox. OpenAI CEO Sam Altman responded on X saying “there was a provision about potential equity cancellation in our previous exit docs; although we never clawed anything back, it should never have been something we had in any documents or communication.”\\nAI DEALS OF THE WEEKAlexandr Wang was just 19 when he started Scale. His cofounder, Lucy Guo, was 21.Scale AI')]\n"
     ]
    }
   ],
   "source": [
    "question = \"Why did OpenAI and Scarlett Johansson have a conflict?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2})\n",
    "search_result = retriever.invoke(question)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a96b86",
   "metadata": {},
   "source": [
    "### Create a variety of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3473f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "question = \"Why did OpenAI and Scarlett Johansson have a conflict?\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e743451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd0aed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_35972\\511090460.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What was the nature of the disagreement between OpenAI and Scarlett Johansson?  ', 'Can you explain the reasons behind the conflict involving OpenAI and Scarlett Johansson?  ', 'What led to the tensions between OpenAI and Scarlett Johansson?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8185cf7",
   "metadata": {},
   "source": [
    "### Ensemble Retriever\n",
    "**BM25 Retriever + Embedding-based Retriever**\n",
    "\n",
    "- `BM25 retriever` (Keyword Search, Sparse Retriever): Based on TF-IDF, considering term frequency and document length normalization.\n",
    "- `Embedding-based retriever` (Contextual Search, Dense Retriever): Transforms text into embedding vectors and retrieves documents based on vector similarity (e.g. cosine similarity, dot product). This reflects the semantic similarity of words.\n",
    "- `Ensemble retriever` : Combines BM25 and embedding-based retrievers to combine the term frequency of keyword searches with the semantic similarity of contextual searches.\n",
    "\n",
    "**Note**\n",
    "\n",
    "TF-IDF(Term Frequency - Inverse Document Frequency) : TF-IDF evaluates words that frequently appear in a specific document as highly important, while words that frequently appear across all documents are considered less important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd02b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52988fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [\n",
    "    \"We saw a seal swimming in the ocean.\",\n",
    "    \"The seal is clapping its flippers.\",\n",
    "    \"Make sure the envelope has a proper seal before sending it.\",\n",
    "    \"Every official document requires a seal to authenticate it.\",\n",
    "]\n",
    "\n",
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(doc_list)\n",
    "bm25_retriever.k = 4\n",
    "\n",
    "faiss_vectorstore = FAISS.from_texts(doc_list, OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6658f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(docs):\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"[{i+1}] {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f046a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "The seal rested on a rock.\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] The seal is clapping its flippers.\n",
      "[2] We saw a seal swimming in the ocean.\n",
      "[3] Every official document requires a seal to authenticate it.\n",
      "[4] Make sure the envelope has a proper seal before sending it.\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] The seal is clapping its flippers.\n",
      "[2] We saw a seal swimming in the ocean.\n",
      "[3] Every official document requires a seal to authenticate it.\n",
      "[4] Make sure the envelope has a proper seal before sending it.\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] The seal is clapping its flippers.\n",
      "[2] We saw a seal swimming in the ocean.\n",
      "[3] Every official document requires a seal to authenticate it.\n",
      "[4] Make sure the envelope has a proper seal before sending it.\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"The seal rested on a rock.\"\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.invoke(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.invoke(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.invoke(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99d87210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "Ensure the package is securely sealed before handing it to the courier.\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] The seal is clapping its flippers.\n",
      "[2] Every official document requires a seal to authenticate it.\n",
      "[3] Make sure the envelope has a proper seal before sending it.\n",
      "[4] We saw a seal swimming in the ocean.\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] Make sure the envelope has a proper seal before sending it.\n",
      "[2] Every official document requires a seal to authenticate it.\n",
      "[3] The seal is clapping its flippers.\n",
      "[4] We saw a seal swimming in the ocean.\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] The seal is clapping its flippers.\n",
      "[2] Make sure the envelope has a proper seal before sending it.\n",
      "[3] Every official document requires a seal to authenticate it.\n",
      "[4] We saw a seal swimming in the ocean.\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"Ensure the package is securely sealed before handing it to the courier.\"\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.invoke(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.invoke(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.invoke(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60a3345b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "The certificate must bear an official seal to be considered valid.\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] Every official document requires a seal to authenticate it.\n",
      "[2] The seal is clapping its flippers.\n",
      "[3] We saw a seal swimming in the ocean.\n",
      "[4] Make sure the envelope has a proper seal before sending it.\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] Every official document requires a seal to authenticate it.\n",
      "[2] Make sure the envelope has a proper seal before sending it.\n",
      "[3] The seal is clapping its flippers.\n",
      "[4] We saw a seal swimming in the ocean.\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] Every official document requires a seal to authenticate it.\n",
      "[2] The seal is clapping its flippers.\n",
      "[3] Make sure the envelope has a proper seal before sending it.\n",
      "[4] We saw a seal swimming in the ocean.\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"The certificate must bear an official seal to be considered valid.\"\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.invoke(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.invoke(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.invoke(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c2210c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "animal\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] Every official document requires a seal to authenticate it.\n",
      "[2] Make sure the envelope has a proper seal before sending it.\n",
      "[3] The seal is clapping its flippers.\n",
      "[4] We saw a seal swimming in the ocean.\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] We saw a seal swimming in the ocean.\n",
      "[2] The seal is clapping its flippers.\n",
      "[3] Every official document requires a seal to authenticate it.\n",
      "[4] Make sure the envelope has a proper seal before sending it.\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] Every official document requires a seal to authenticate it.\n",
      "[2] We saw a seal swimming in the ocean.\n",
      "[3] The seal is clapping its flippers.\n",
      "[4] Make sure the envelope has a proper seal before sending it.\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"animal\"\n",
    "\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.invoke(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.invoke(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.invoke(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9d842",
   "metadata": {},
   "source": [
    "## Step 6: Create Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d94fcc",
   "metadata": {},
   "source": [
    "Prompt engineering plays a crucial role in deriving the desired outputs based on the given data( `context` ) .\n",
    "\n",
    "[TIP1]\n",
    "\n",
    "1. If important information is missing from the results provided by the `retriever `, you should modify the `retriever` logic.\n",
    "2. If the results from the `retriever` contain sufficient information, but the llm fails to extract the key information or doesn't produce the output in the desired format, you should adjust the prompt.\n",
    "\n",
    "[TIP2]\n",
    "\n",
    "1. LangSmith's **hub** contains numerous verified prompts.\n",
    "2. Utilizing or slightly modifying these verified prompts can save both cost and time.\n",
    "\n",
    "- https://smith.langchain.com/hub/search?q=rag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20486a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61e05003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m \n",
      "Context: \u001b[33;1m\u001b[1;3m{context}\u001b[0m \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d8714",
   "metadata": {},
   "source": [
    "## Step 7: Create LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f99381",
   "metadata": {},
   "source": [
    "Select one of the OpenAI models:\n",
    "\n",
    "- `gpt-4o` : OpenAI GPT-4o model\n",
    "- `gpt-4o-mini` : OpenAI GPT-4o-mini model\n",
    "\n",
    "For detailed pricing information, please refer to the [OpenAI API Model List / Pricing](https://openai.com/api/pricing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85dcaa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64e9a5",
   "metadata": {},
   "source": [
    "You can check token usage in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f087fd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 24\n",
      "\tPrompt Tokens: 15\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 9\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $7.65e-06\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = model.invoke(\"Where is the capital of South Korea?\")\n",
    "print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6cd40",
   "metadata": {},
   "source": [
    "You can easily download and use open-source models available on HuggingFace.\n",
    "\n",
    "You can also check the open-source leaderboard, which improves performance daily, at the link below:\n",
    "\n",
    "- [HuggingFace LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "\n",
    "**Note**\n",
    "\n",
    "Hugging Face's free API has a 10GB size limit.\n",
    "For example, the google/flan-t5-xl model is 11GB, making it inaccessible via the free API.\n",
    "\n",
    "Choose one of the options below:\n",
    "\n",
    "1. Option: Use Hugging Face Inference Endpoints\n",
    "\n",
    "Activate Inference Endpoints through a paid plan to perform large-scale model inference.\n",
    "\n",
    "2. Option: Run the model locally\n",
    "\n",
    "Use the transformers library to run the flan-t5-xl model in a local environment (GPU recommended).\n",
    "\n",
    "3. Option: Use a smaller model (e.g. flan-t5-large)\n",
    "\n",
    "Reduce the model size to one supported by the free API and execute it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba46d84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_35972\\2857973467.py:6: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  t5_model = HuggingFaceHub(\n"
     ]
    }
   ],
   "source": [
    "# Creating a HuggingFaceHub object\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "repo_id = \"google/flan-t5-large\"\n",
    "\n",
    "t5_model = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_length\": 512}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "628f91f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seoul'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model.invoke(\"Where is the capital of South Korea?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74046f",
   "metadata": {},
   "source": [
    "## RAG Template Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "88c64211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Path: data/A European Approach to Artificial Intelligence - A Policy Perspective.pdf\n",
      "Number of documents: 86\n",
      "============================================================\n",
      "[HUMAN]\n",
      "Which region's approach to artificial intelligence is the focus of this document?\n",
      "\n",
      "[AI]\n",
      "The focus of this document is on the European approach to artificial intelligence. It discusses various initiatives and strategies put forth by the European Commission to enhance AI development and governance within Europe. The document emphasizes collaboration among EU member states to foster AI innovation \"made in Europe.\"\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Documents\n",
    "# Load the documents, split them into chunks, and index them.\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Load the PDF file. Enter the file path.\n",
    "file_path = \"data/A European Approach to Artificial Intelligence - A Policy Perspective.pdf\"\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "\n",
    "# Step 2: Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "split_docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "# Step 3, 4: Embeding & Create Vectorstore\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = FAISS.from_documents(documents=split_docs, embedding=embedding)\n",
    "\n",
    "# Step 5: Create Retriever\n",
    "# Search for documents that match the user's query.\n",
    "\n",
    "# Retrieve the top K documents with the highest similarity.\n",
    "k = 3\n",
    "\n",
    "# Initialize the (Sparse) BM25 retriever and (Dense) FAISS retriever.\n",
    "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
    "bm25_retriever.k = k\n",
    "\n",
    "faiss_vectorstore = FAISS.from_documents(split_docs, embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# Step 6: Create Prompt\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Step 7: Create LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # Combine the retrieved document results into a single paragraph.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Step 8: Create Chain\n",
    "rag_chain = (\n",
    "    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run Chain: Input a query about the document and output the answer.\n",
    "\n",
    "question = \"Which region's approach to artificial intelligence is the focus of this document?\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# Get Output\n",
    "print(f\"PDF Path: {file_path}\")\n",
    "print(f\"Number of documents: {len(split_docs)}\")\n",
    "print(\"===\" * 20)\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e349c363",
   "metadata": {},
   "source": [
    "Document: A European Approach to Artificial Intelligence - A Policy Perspective.pdf\n",
    "\n",
    "- LangSmith: https://smith.langchain.com/public/0951c102-de61-482b-b42a-6e7d78f02107/r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1bc2c56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The focus of this document is on the European approach to artificial intelligence. It discusses various initiatives and strategies put forth by the European Commission to enhance AI development and governance within Europe. The document emphasizes collaboration among EU member states to foster AI innovation \"made in Europe.\"\n"
     ]
    }
   ],
   "source": [
    "question = \"Which region's approach to artificial intelligence is the focus of this document?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc31d23",
   "metadata": {},
   "source": [
    "Document: A European Approach to Artificial Intelligence - A Policy Perspective.pdf\n",
    "\n",
    "- LangSmith: https://smith.langchain.com/public/c968bf7e-e22e-4eb1-a76a-b226eedc6c51/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "218d2974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The primary principle of the European AI approach is to place people at the center of AI development, often referred to as \"human-centric AI.\" This approach aims to support technological and industrial capacity, prepare for socio-economic changes, and ensure an appropriate ethical and legal framework. It emphasizes the need for AI to comply with the law, fulfill ethical principles, and be robust to achieve \"trustworthy AI.\"\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the primary principle of the European AI approach?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b833cf",
   "metadata": {},
   "source": [
    "Ask a question unrelated to the document.\n",
    "\n",
    "- LangSmith: https://smith.langchain.com/public/d8a49d52-3a63-4206-9166-58605bd990a6/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "265b7d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The retrieved context does not provide specific information about the obligations of the United States in AI. Therefore, I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the obligation of the United States in AI?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e153841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
